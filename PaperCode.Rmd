---
title: "Untitled"
author: "Jonathan Bourne"
date: "9 August 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---


schedule 2 part 5 section 


```{r}
SubCode <- "~/Dropbox/SSE/Empty Homes/EmptyHomesCode/SubCode"
setwd(SubCode)
source("Setup.R")

setwd(Functions)
list.files() %>% map(source)

Figures <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Figures" #file.path(basewd, "Figures")
TexTables <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Tables"
suppressMessages(source(file.path(CommonCode, "AuxdataLoad.R")))
```


#Process all areas

This chunk processes all the areas and combines into a single dataframe
It also cleans all postcodes that are in other LADs out... should it just combine them into the other LADs LSOA?
```{r}
#creates a data frame called DATAdf
suppressMessages(source(file.path(SubCode, "LOADandProcessLADData.R")))


```

#MSOA to LAD
```{r}
MSOAtoLAD <- EW2 %>%
  group_by(MSOA11CD, LAD11CD, LAD11NM) %>%
  summarise(TotMSOA = n(),
         Homes = sum(Homes),
         Pop = sum(Pop),
         Region = first(Region)) %>%
  arrange(-TotMSOA) %>%
  group_by(MSOA11CD) %>%
  mutate(counts = n()) %>%
  summarise(LAD11CD = first(LAD11CD),
            TotMSOA = first(TotMSOA),
            counts = first(counts),
            LAD11NM = first(LAD11NM),
            Homes = first(Homes),
            Pop = first(Pop),
            Region = first(Region)) %>%
  mutate(Region = ifelse(is.na(Region), "Wales", Region))
```


#Get Population Statistics

This chunk calculates how much data cover I have using several metrics

```{r}
DataCover <-DATAdf  %>%
                #filter(is.na(MSOA11CD)) %>%
  summarise(
           'LADs' = length(unique(DATAdf$LAD11CD)), 
           '% of all LADs' = length(unique(DATAdf$LAD11CD))/length(unique(EW2$LAD11CD))*100,
           LUPs = sum(LowUse, na.rm = T),
           Homes = sum(Homes, na.rm = T),
           '% of all Homes' = Homes/sum(EW2$Homes)*100,
           Population = sum(Pop, na.rm= T),
           '% of total population' = Population/sum(EW2$Pop)*100) %>%
  mutate_if(.<1, funs(signif(.,3))) %>%
  mutate_if(.>1, as.integer) %>%
  mutate_all(as.character) %>% 
  gather 

DataCover 

xtable(DataCover, caption="Summary of collected data coverage",
       label = "tab:DataCover") %>% 
  print(., type="latex", file=file.path(TexTables,"DataCover.tex"))

rm(DataCover)

#lost LUPS

LostLowUse <- DATAdf %>%
  filter(is.na(MSOA11CD)) %>%
  group_by(LAD11CD) %>%
  summarise(Lost = sum(LowUse)) %>%
  arrange(-Lost) %>%
  mutate(cumsum = cumsum(Lost),
         cumperc = cumsum/sum(Lost))

sum(LostLowUse$Lost)
```


#Calculate Quantiles

This rather long winded way of getting the quartiles is important as it weights the sampling by Homes in an area. Relative proportion of sales in an area is not the same as homes. Some areas have higher turnover other lower. This means the quartiles need to be repeatedly sampled to get the right quartiles for the whole dataset.

In the below chunk I have sampled the data 501 times, however it is only necessary to sample one the quartiles don't change.

```{r}

#Create a data frame of LSO
PriceCounts <-prices %>%
  filter(X5 %in% c("D", "S", "T", "F")) %>%
  group_by(LSOA11CD) %>%
  summarise(counts = n()) %>%
  arrange(counts)
##The subsampling is 1/10 of the number of homes in each LSOA, this stops crashes
PriceCounts<- EW2 %>%
  select(LSOA11CD, Homes, Pop, LAD11CD, MSOA11CD) %>%
  left_join(.,PriceCounts, by = "LSOA11CD") %>%
  mutate(counts = ifelse(is.na(counts), 0, counts),
         Homes2 = Homes/10) %>%
  filter(counts!=0) %>%
  left_join(IncomeEst %>% select(MSOA11CD, Yearly.income))

#Create a list of price vectors by LSOA
#This process can take a while
setwd(DataFolder)
if(file.exists("LSOAPriceList2.rds")){
  
  LSOAPriceList <- readRDS("LSOAPriceList2.rds")
  
}else{
   
  LSOAPriceList <- 1:nrow(PriceCounts) %>% map(~{
   print(.x)
    prices %>%
      filter(LSOA11CD==PriceCounts$LSOA11CD[.x], 
             X5 %in% c("D", "S", "T", "F")) %>%
      pull(X2)
    })
  names(LSOAPriceList) <- PriceCounts$LSOA11CD
  saveRDS(LSOAPriceList, "LSOAPriceList2.rds")

}


 #Price Quartiles
 if(file.exists("AllDataQuartiles.rds")){
  
  AllDataQuartiles <- readRDS("AllDataQuartiles.rds")
  
}else{
  #Subset list to only the LADs used in analyis
  LSOAPriceList2 <- LSOAPriceList[names(LSOAPriceList) %in% unique(DATAdf$LSOA11CD)]
  PriceCounts2 <- PriceCounts %>%
    filter(LSOA11CD %in% unique(DATAdf$LSOA11CD))
  
  AllDataQuartiles <- StratifiedBoot(LSOAPriceList2, 
                                     PriceCounts2$LSOA11CD, 
                                     PriceCounts2$Homes2, 
                                     samples = 100) #change to 1 for a quicker solution has no effect on outcome
  
  saveRDS(AllDataQuartiles, "AllDataQuartiles.rds")
  
  rm(list = c("LSOAPriceList2", "PriceCounts2"))

}

AllDataQuartilesSummary <- AllDataQuartiles %>%
  summarise_all(funs(mean, sd))

BasicQuartiles <- prices$X2 %>% .[prices$LAD11CD %in% unique(DATAdf$LAD11CD)] %>% quantile(.)

#difference between the two up to 8% this has knock on effects later in the analysis if the quartiles are not calculated correctly
AllDataQuartilesSummary[2:4]/BasicQuartiles[2:4]

#Add the balanced quartiles on to the prices.
prices <- prices %>%
  mutate(CountryClass = cut(X2, AllDataQuartilesSummary[2:4] %>% c(0,., Inf), 
                              labels =     c("Lower", "Lower-Mid", "Upper-Mid", "Upper"), 
                              right = F) %>% fct_relevel(., "Upper", after = 3)) 

#Affordability Quartiles

if(file.exists("LSOAAffordList.rds")){
  
  LSOAAffordList <- readRDS("LSOAAffordList.rds")
  
}else{
   
  LSOAAffordList<- 1:nrow(PriceCounts) %>%
  map(~{
    print(.x)
    LSOAPriceList[[.x]]/PriceCounts$Yearly.income[.x]
        
  })
 names(LSOAAffordList) <- PriceCounts$LSOA11CD
  saveRDS(LSOAAffordList, "LSOAAffordList.rds")

}


 #Price Quartiles
 if(file.exists("AllDataAffordQuartiles.rds")){
  
  AllDataAffordQuartiles <- readRDS("AllDataAffordQuartiles.rds")
  
}else{
  #Subset list to only the LADs used in analyis
  LSOAAffordList2 <- LSOAAffordList[names(LSOAAffordList) %in% unique(DATAdf$LSOA11CD)]
  PriceCounts2 <- PriceCounts %>%
    filter(LSOA11CD %in% unique(DATAdf$LSOA11CD))
  
   AllDataAffordQuartiles <- StratifiedBoot(LSOAAffordList2, 
                                     PriceCounts2$LSOA11CD, 
                                     PriceCounts2$Homes2, 
                                     samples = 100) #change to 1 for a quicker solution has no effect on outcome
  
  saveRDS(AllDataAffordQuartiles, "AllDataAffordQuartiles.rds")
  
  rm(list = c("LSOAAffordList2", "PriceCounts2"))

}

AllDataAffordQuartilesSummary <-  AllDataAffordQuartiles %>%
  summarise_all(funs(mean, sd))


#Add the balanced quartiles on to the prices.
prices <- prices %>%
  left_join(select(PriceCounts, LSOA11CD, Yearly.income)) %>%
  mutate(Affordability = X2/Yearly.income) %>%
  select(-Yearly.income ) %>%
  mutate(CountryAffordClass = cut(Affordability, AllDataAffordQuartilesSummary[2:4] %>% c(0,., Inf), 
                              labels =     c("Lower", "Lower-Mid", "Upper-Mid", "Upper"), 
                              right = F) %>% fct_relevel(., "Upper", after = 3)) 

rm(BasicQuartiles);rm(AllDataQuartiles);rm(AllDataQuartilesSummary);rm(LSOAPriceList);rm(AllDataAffordQuartiles);rm(AllDataAffordQuartilesSummary);rm(LSOAAffordList)
   
```


#BootStrap

This section bootstraps the price classes for each property then aggregates it by quartiles per LAD
From this we can find out if the majority of a LADs Low Use properties are investment or not by comparing the median values.

The same can be done at MSOA level but the areas are so small that you don't get enough variation in the house price levels to get a meaningful result. A better way to do this would be to use the ego network of MSOA around a target to find the "Neighbourhood LAD" and see if that is investment. However as I don't have the the low use property data for all adjoining MSOA this is not possible.

##Quartiles Bootstrap

Needed for creating the skew plots
```{r}
setwd(file.path(basewd, "BootstrapQuartiles"))
BootstrapAllData(DATAdf, LimitValue = 1.4e5, Reps = 501)
BootStrapQuartiles <- LoadBootstrapData()

```

Affordability Bootstrap
```{r}
setwd(file.path(basewd, "BootstrapAffordQuartiles"))
BootstrapAllData(DATAdf, LimitValue = 1.4e5, Reps = 501, GroupingVars = c("AffordClass", "CountryAffordClass"))
#BootStrapAffordQuartiles <- LoadBootstrapData()
#BootStrapQuartiles2 <- BootStrapQuartiles

BootStrapQuartiles<- BootStrapAffordQuartiles
```



##MSOA Bootstrap
Used for most of the analysis, doesn't need to be loaded as it is loaded by other functions
```{r}
setwd(file.path(basewd, "BootstrapMSOA"))
BootstrapAllData(DATAdf, LimitValue = 1.4e5, Reps = 501, GroupingVars = "MSOA11CD")

```

##Non-Dataset LADs
This bootstrapping is done to get the data needed to predict across the whole of Wngland and Wales
```{r}
#Filter out all the LADs for the data we have, ake dataframe that can be sampled
AllOtherLADsdf <- EW2 %>%
  filter(!(LAD11CD %in% DATAdf$LAD11CD)) %>%
  select(LSOA11CD:Pop, MSOA11CD, LAD11CD, LAD11NM)%>%
  mutate(LowUse = 0) 
#Bootstrap
setwd(file.path(basewd, "BootstrapAllLADs"))
BootstrapAllData(AllOtherLADsdf, LimitValue = 1.4e5, Reps = 501, GroupingVars = "MSOA11CD")

#Not needed again
rm(AllOtherLADsdf)

```

##Old prices Bootstrap

This is to calculate % change
```{r}

if(length(list.files(file.path(basewd, "BootstrapMSOAOldPrices")))==0){
prices <- prices2
DATAdf2 <- DATAdf %>% mutate(LowUse = 0)
prices <- readRDS(file.path(DataFolder,"pricesOld.rds")) %>% mutate(CountryClass = 1, Affordability = 1, CountryAffordClass = 1)
setwd(file.path(basewd, "BootstrapMSOAOldPrices"))
BootstrapAllData(DATAdf2, LimitValue = 1.4e5, Reps = 501, GroupingVars = "MSOA11CD")
rm(DATAdf2)
prices<- prices2
rm(prices2)
}

```


#Low Use quartile plot

This chunk plots which of the LADs affordability quartiles the LUPs fall into.

```{r}

setwd(file.path(basewd, "BootstrapAffordQuartiles"))
BootStrapQuartiles <- LoadBootstrapData()

#Plot for all data
All_data_class <- BootStrapQuartiles %>% 
  bind_rows %>%
  select(CountryClass = CountryAffordClass, ID, LowUse, Homes)

#All data summary
All_data_class %>% 
  group_by(Class,ID) %>%
  summarise_all(sum) %>%
  group_by(Class) %>%
  summarise_all(mean) %>%
  CleanForPlotClassDiff()
  
#Create multi panel plot for two areas and all data
bind_rows(
CleanForPlotClassDiff(BootStrapQuartiles$E09000020, AffordClass) %>% mutate(Area = "Kensington and Chelsea"),
CleanForPlotClassDiff(BootStrapQuartiles$E08000032, AffordClass) %>% mutate(Area = "Bradford"),
CleanForPlotClassDiff(All_data_class, CountryClass) %>% mutate(Area = "All Data")
) %>%
  mutate(Area = factor(Area, levels = c("Bradford", "All Data", "Kensington and Chelsea"))) %>%
  PlotClassDiff(Var = "LowUseRatio1") +
    facet_wrap(.~Area) +
  ggtitle("Difference in Low-Use Property distribution by Price Quartile")
SaveFig("ClassdiffAllData.pdf")

rm(BootStrapQuartiles);rm(All_data_class)
```

#Tourism data
This section Loads and processes tourism Data


```{r}
if(file.exists(file.path(DataFolder, "Tourism.rds"))){
  
  Tourism <- readRDS(file.path(DataFolder, "Tourism.rds"))
}else{
  
  suppressMessages(source(file.path(SubCode, "LoadProcessTourism.R")))

}
```


```{r}
test <- list.files(file.path(basewd, "BootstrapMSOAOldPrices"), full.names = TRUE) %>%
    map_df(~readRDS(.x)) %>% select(MSOA11CD,LAD11CD, ID, MSOAHomesMedian:LADHomesMean) %>% 
  set_names(c(names(.)[1:3], paste0(names(.)[-c(1:3)], "Old")))
  
  group_by(LAD11CD) %>%
      summarise(HomesMedian = first(LADHomesMedian),
                HomesMean = first(LADHomesMean)) %>% 
  rename(LADHomesMedianOld = HomesMedian,
         LADHomesMeanOld = HomesMean)

test2 <- left_join(LADMean, test) %>%
  mutate(PercChange = (HomesMedian-LADHomesMedianOld)/LADHomesMedianOld,
         PercChange = (HomesMean-HomesMeanOld)/HomesMeanOld)

t.test(test2$LADHomesMedianOld, test2$HomesMedian, alternative = "less")


test2 %>%
  ggplot(aes(x = HomesMedian, y = LADHomesMedianOld))

test2 %>%
  ggplot(aes(x = PercChange)) + geom_density()

test2%>%
  ggplot(aes(x = PercChange, y = AffordRatio)) + geom_point()
  
cor(test2$AffordRatio, test2$PercChangeMean)


MSOAtest <- list.files(file.path(basewd, "BootstrapMSOAOldPrices"), full.names = TRUE) %>%
    map_df(~readRDS(.x)) %>% select(MSOA11CD,LAD11CD, ID, MSOAHomesMedian:LADHomesMean) %>% 
   group_by(MSOA11CD) %>%
      summarise(HomesMedianOld = first(MSOAHomesMedian),
                HomesMeanOld = first(MSOAHomesMean)) %>% 
  left_join(MSOAMean, .) %>%
  mutate(PercChange = (HomesMedian-HomesMedianOld)/HomesMedianOld,
         PercChangeMean = (HomesMean-HomesMeanOld)/HomesMeanOld)

cor(MSOAtest$AffordRatio, MSOAtest$PercChangeMean)


MSOAtest %>%
  ggplot(aes(x = PercChange)) + geom_density()


MSOAtest%>%
  ggplot(aes(x = log(TourismDensity+1), y = percent_rank(PercChange), colour = HighLUP)) + geom_point()


```


#Create LAD and MSOA DATA frames

This chunk creates useful data frames for the rest of the analysis

##LAD Data

```{r}
LADModelData <-CreateGeogModelData("LAD", file.path(basewd, "BootstrapMSOA"))
LADMean <- MeanModelData(LADModelData, "LAD") %>%
  mutate(TourismDensityRank = rank(TourismDensity, ties.method = "max"),
         TourismDensity = TourismDensity*1000, #Makes the coefficients easier
         LowUseType =case_when(
    "TRUE"==HighLUP & "TRUE" ==HighVal ~ 1, # All high
    "TRUE" == HighLUP & "FALSE" ==HighVal ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & "TRUE" ==HighVal ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & "FALSE"==HighVal ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
  LowUseAfford =case_when(
    "TRUE"==HighLUP & AffordRank>=0.5 ~ 1, # All high
    "TRUE" == HighLUP & AffordRank<0.5 ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & AffordRank>=0.5 ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & AffordRank<0.5 ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
    HighFinance = factor("TRUE"==HighVal & "TRUE"==HighLUP, levels = c("FALSE", "TRUE"))   )  %>%
  left_join(MSOAtoLAD %>%
              group_by(LAD11CD) %>%
              summarise(LAD11NM = first(LAD11NM)), by = "LAD11CD")


#example Affordatio bootstrap data for Chelsea. Variaince is very low
LADModelData %>% filter(LAD11CD=="E09000020") %>% ggplot(aes(x = AffordRatio)) + geom_density()

  LADMean %>%
  ggplot(aes(x= AffordRank, y = LowUseRank, colour = LowUseAfford)) + geom_point() +
    geom_vline(xintercept  = 0.5)+
    geom_hline(yintercept = 0.5) +
    labs(title = "Seperating Low-Use type  by affordability", x = "Affordability percentage rank", y = "Low-Use percentile rank")
  #SaveFig("LowUseAffordcolour.pdf")
 
 
  #Relationship between Price change and affordability
#From this we can see that areas with the most growth are the most unaffordable
      cor(LADMean$PercChange, LADMean$AffordRatio)
#However this only really tells us that the most expensive areas previously continue to be the most expensive  
      cor(LADMean$HomesMeanOld, LADMean$HomesMean)

```


##MSOA Data

```{r}
MSOAModelData <-CreateGeogModelData("MSOA", file.path(basewd, "BootstrapMSOA")) 
MSOAMean <- MeanModelData(MSOAModelData, "MSOA") %>%
  left_join(select(MSOAtoLAD, MSOA11CD, LAD11CD)) %>%
  mutate(AffordRank2 = scale(AffordRank)^2,
         AffordRatio2 = scale(AffordRatio)^2,
         TourismDensityRank = rank(TourismDensity, ties.method = "max"),
         TourismDensity = TourismDensity*1000) %>% #Makes the coefficients easier
  left_join(select(LADMean, LAD11CD, LADHighLUP = HighLUP, LADHighVal = HighVal, #Add in Saucy LAD variables
                   LADAffordRatio = AffordRatio, LADAffordRatio2 = AffordRatioScale2,
                   LADTourismDensity = TourismDensity, LADMedianDiff = MedianDiff, LADMeanMedianRatio = MeanMedianRatio,
                   LowUseType), by = "LAD11CD") %>%
  mutate(BothHighLUP =case_when(
    "TRUE"==HighLUP & "TRUE" ==LADHighLUP ~ 1, # all high
    "TRUE" == HighLUP & "FALSE" ==LADHighLUP ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & "TRUE" ==LADHighLUP ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & "FALSE"==LADHighLUP ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
  BothHighVal =case_when(
    "TRUE" ==HighVal & "TRUE" ==LADHighVal ~ 1, # all high
   "TRUE" == HighVal & "FALSE"==LADHighVal ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighVal & "TRUE" ==LADHighVal ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighVal & "FALSE"==LADHighVal ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
  MSOALUPLADVal =case_when(
    "TRUE" ==HighLUP & "TRUE" ==LADHighVal ~ 1, # all high
   "TRUE" == HighLUP & "FALSE"==LADHighVal ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & "TRUE" ==LADHighVal ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & "FALSE"==LADHighVal ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
       HighFinance = factor("TRUE"==LADHighVal & "TRUE"==HighLUP, levels = c("FALSE", "TRUE"))     )
 

  #Relationship between Price change and affordability
#From this we can see that areas with the most growth are the most unaffordable
      cor(MSOAMean$PercChange, MSOAMean$AffordRatio)
#However this only really tells us that the most expensive areas previously continue to be the most expensive  
      cor(MSOAMean$HomesMeanOld, MSOAMean$HomesMean)

```
 
 
Plots that help explain the data seperation

```{r}
bind_rows(MSOAMean %>% mutate(type = "MSOA"), LADMean %>% mutate(type = "LAD"))  %>%group_by(type)%>% mutate(Eg2 = percent_rank(Ego2TourismDens)) %>% ungroup %>%
  ggplot(aes(x= AffordRank, y = log(TourismDensity), colour = HighLUP)) + geom_point() +
  facet_wrap(~type) + 
  labs(title = "Does the geography have above the median LUPs", x = "Affordability Rank", y = "Tourism Rank") +
  guides(fill=guide_legend(title="Above Median"))
SaveFig("HighLUP.pdf")

bind_rows(MSOAMean %>% mutate(type = "MSOA"), LADMean %>% mutate(type = "LAD")) %>%
  ggplot(aes(x= AffordRank, y = log(TourismDensity), colour = HighVal)) + geom_point() +
  facet_wrap(~type) + 
  labs(title = "Is the median LUP price higher than the Median price of all properties", x = "Affordability Rank", y = "Tourism Density") +
  guides(fill=guide_legend(title="Above Median"))
SaveFig("HighVal.pdf")

quantile(MSOAMean$LowUsePerc)

```

#Empty Homes tax

Calculates the vancouver tax

welsh data comes from 
https://statswales.gov.wales/Catalogue/Local-Government/Finance/Council-Tax/Levels/counciltaxlevels-by-billingauthority-band



```{r}
setwd(DataFolder)
WelshTax <- read_csv("WelshCTax.csv", skip = 6, n_max = 23) %>%
  select(-X1) %>%
  mutate(CouncilTax = rowSums(.[-1])*1000) %>%
  rename(LAD11NM = X2) %>%
  left_join(., MSOAtoLAD %>% 
              select(LAD11CD, LAD11NM) %>%
              distinct(LAD11NM, .keep_all = T)) %>%
  filter(!is.na(LAD11CD)) %>%
    select(LAD11CD, CouncilTax)

QRC2017 <- read_xlsx("QRC4-2017-2018.xlsx", sheet = 2, skip = 2) %>%
  select(3:4) %>%
  set_names("LAD11CD", "CouncilTax") %>%
  mutate(CouncilTax = CouncilTax*1000) %>%
  bind_rows(WelshTax)

EmptyTax <- LADMean %>% 
  left_join(., QRC2017)%>%
  left_join(MSOAtoLAD %>%
              group_by(LAD11CD) %>%
              summarise(Pop = sum(Pop))) %>%
  mutate(EmptyTax = LowUse*LowUseMean*.01,
         EmptyOfCouncil = EmptyTax/CouncilTax,
         TaxPop = EmptyTax/Pop) 

sum(EmptyTax$EmptyTax)/1e6
mean(EmptyTax$EmptyOfCouncil)
median(EmptyTax$EmptyOfCouncil)

mean(EmptyTax$EmptyTax)
median(EmptyTax$EmptyTax)

sum(EmptyTax$EmptyTax)/sum(EmptyTax$CouncilTax)

EmptyTax %>%
  ggplot(aes(x =EmptyOfCouncil)) + geom_density() +
  coord_cartesian(x=c(0,0.7)) +
  scale_x_continuous(labels = scales::percent) +
  labs(title = "Distribution of value of Empty homes tax in terms of council tax by local authority", 
       x = "Percent of council tax value")


EmptyTax %>%
  select(EmptyOfCouncil, LAD11NM) %>%
  arrange(EmptyOfCouncil) %>%
  top_n(5,EmptyOfCouncil)


rm(WelshTax);rm(QRC2017)

```


#Rural Analysis

The rural areas seem to be much less likely to had a negative mean difference,  but there is not enough data for the data to be used for moddelling
```{r}
LADrural <- rural %>% group_by(LAD11CD, Urban) %>%
  summarise(count = n()) %>%
  spread(key = Urban, value = count, fill = 0) %>%
  mutate(UrbanPerc = Urban/(Urban+Rural),
         Class = ifelse(UrbanPerc>0.5, "Urban", "Rural")) %>%
  left_join(LADMean,.)


MSOArural <- rural %>% group_by(MSOA11CD, Urban) %>%
  summarise(count = n()) %>%
  spread(key = Urban, value = count, fill = 0) %>%
  mutate(UrbanPerc = Urban/(Urban+Rural),
         Class = ifelse(UrbanPerc>0.5, "Urban", "Rural")) %>%
  left_join(MSOAMean,.)

#Almost no rural areas are classified as having a negative median difference
table(LADrural$Class,LADrural$MedianDiff>0.5)
table(LADrural$Class,LADrural$LowUseType)
2/26
42/86

table(ifelse(percent_rank(LADrural$TourismDensity)>0.5, "HighTour", "LowTour"),LADrural$HighLUP,LADrural$Class)

LADrural  %>%
  ggplot(aes(x = log(LowUsePerc), y =  log(TourismDensity), colour = Class)) + geom_point()


rm(LADrural);rm(MSOArural);rm(rural)
```


#Affordability

This chunk includes affordability and plots the theoretical and observed curves.
Uses the Bootstrapped values to calculate the affordability

```{r}
MSOAAffordRatio <- MSOAMean %>%
  left_join(select(MSOAtoLAD, MSOA11CD, Region), by ="MSOA11CD")

cor(MSOAMea)

  
MSOAAffordRatio  <- bind_rows(MSOAAffordRatio %>% filter(Region != "London") %>% mutate(type = "Excluding London"), 
                    MSOAAffordRatio %>% mutate( type = "Including London"),
                    MSOAAffordRatio %>% filter(Region == "London") %>% mutate( type = "Only London")) %>%
    group_by(type) %>%
    mutate(
      decile = ntile(AffordRatio, 100)) %>%
      group_by(decile, type) %>%
    summarise(mean = mean(LowUsePerc),
              median = median(LowUsePerc),
              counts = n(),
              AffordRatio= mean(AffordRatio)
              ) 


MSOAAffordRatio %>% 
  #filter(type == "Excluding London") %>% 
  ggplot(aes(x= decile, y = median)) + 
  facet_grid(.~type) + 
  #geom_point(data =unaffordExLon , aes(x= decile, y = LowUsePerc), alpha = 0.2 )+
  geom_point(aes(colour = type))  +
  labs(x = "2 percentile group", y = "median Percent Low Use") + 
  theme(legend.position= "none") + 
  geom_smooth(method="loess",se=FALSE, colour = "black") +
 #coord_cartesian(ylim = c(0.01, 0.05)) +
   scale_y_continuous(labels = scales::percent)
SaveFig("LowUsePercVSratio.pdf")


#The plot of the theoretical curves
data_frame(Index = 1:100) %>%
  mutate(Primary = exp(-0.05*(Index))+0.05, 
         Auxilary = (exp(Index/100)-1),
    Total = Primary + Auxilary) %>%
  gather(key = "Type", value = "value", -Index) %>%
  ggplot(aes(x= Index, y = value, colour = Type )) +
  geom_line() +
  labs(title = "LUP percentage as a function of primary and Auxilary demand",
       x = "Demand",
       y = "LUP percentage") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank()
  )
SaveFig("TheoreticalDemand.pdf")

```



#Models

The models that will be made are all binary logistic regression

Are the majority of the of the LUPS above the LAD price? 
  this uses the variables Affordability Rank (normalised), the square of the affordability rank and tourism
Are there more than the average LUPS in the LAD

Are there more than average LUPS in the LSOA/MSOA

Both models can be bootstrapped as the investment number relies on the inferred value of Homes and LUPs and the affordability relies on the bootstrapped Homes price... only question is is there enough variation in the median homes price for it to be worth it.

##Model Parameters

The below are the combinations of formula and outcome variable

Including Mean Median is useful for LADs where the intra lad variance is large but not for MSOA where the whole area can have a high number of LUPs not just a part which occurs in lads. The effect on the lad model is to increase accuracy by only 1% 

```{r}
Formulas <- c(as.formula(Reference ~ AffordRatio),
              as.formula(Reference ~ TourismDensity + AffordRatio),
              as.formula(Reference ~ AffordRatio + AffordRatioScale2),
              as.formula(Reference ~ TourismDensity+ AffordRatio + AffordRatioScale2))

#the below function is used to clearly mark the model summaries with the formular type
MarkModels <- function(df){
  df %>%
  mutate(IncAfford2 = grepl("AffordRatioScale2", Formula),
        # IncAfford3 = grepl("3", Formula),
           IncTourism = grepl("Tourism", Formula)
         )
  
}

```



##LAD Model

```{r}
#Create sample
set.seed(1235)
LADHighValResample <- vfold_cv(LADMean, v = 5, repeats = 20, strata = "HighVal")
set.seed(1235)
LADHighLUPResample <- vfold_cv(LADMean, v = 5, repeats = 20, strata = "HighLUP")
set.seed(1235)
LADHighFinanceResample <- vfold_cv(LADMean, v = 5, repeats = 20, strata = "HighFinance")


#Create a load of classifiers
LADModels <- bind_rows(LinearClassifierCombiner(LADMean,  "HighVal", Formulas, LADHighValResample),
                       LinearClassifierCombiner(LADMean,  "HighLUP", Formulas, LADHighLUPResample),
                       LinearClassifierCombiner(LADMean,  "HighFinance", Formulas, LADHighFinanceResample)) %>%
MarkModels

#Create a load of linear models
LADModelsLin <- bind_rows(LinearClassifierCombiner(LADMean,  "LowUsePerc", Formulas, LADHighValResample, Classifier = FALSE),
                       LinearClassifierCombiner(LADMean,  "MedianDiff", Formulas, LADHighValResample, Classifier = FALSE)) %>%
MarkModels()

LADModTab <-LADModels %>% SummariseModels(Dependent, IncAfford2,  IncTourism) %>%
 # mutate(Dependent = ifelse(Dependent=="HighLUP", "High LUP %", "High LUP value")) %>%
  mutate(ModelID = rep(1:4,3))

LADModTab %>% select(ID = ModelID, Dependent, `Beats NULL`, `Mean Accuracy`, PosPred, NegPred, `Null Difference`) %>%
  slice(c(2,8,10)) %>%
  xtable(., caption="Summary of viable Local authority models",
       label = "tab:LADmodels") %>% 
  print(., type="latex", file=file.path(TexTables,"LADmodels.tex"))

#Linear models are rubbish
LADModelsLin %>%
  select(-sample) %>%
  group_by(Dependent, IncAfford2, IncTourism) %>%
  summarise_if(is.numeric, funs(mean))

#Formula two provides the most accurate model without over fitting. Being a good model for identifying which LADs will have high value LUPs
LADMODOut <- ExtractClassCoeffs(LADMean, "HighVal", LADHighValResample, Formulas[[2]]) %>% filter(term !="(Intercept)")
LADMODOut %>% ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
LADMODOut %>% ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

#Explore the best classifier
#There The quadtratic relationship is accurately covered using this formula 
LADMODOut2 <- ExtractClassCoeffs(LADMean, "HighLUP", LADHighLUPResample, Formulas[[4]]) %>% filter(term !="(Intercept)")
LADMODOut2 %>%  ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
LADMODOut2 %>%  ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

LADMODOut %>%  select(-statistic, -std.error) %>% group_by(term) %>% summarise_all(funs(mean, median, sd))
LADMODOut2 %>%  select(-statistic, -std.error) %>% group_by(term) %>% summarise_all(funs(mean, median, sd))


LADMODOut3 <- ExtractClassCoeffs(LADMean, "HighFinance", LADHighLUPResample, Formulas[[2]]) %>% filter(term !="(Intercept)")
LADMODOut3 %>%  ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
LADMODOut3 %>%  ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

```


##MSOA Model
```{r}

set.seed(1235)
MSOAHighValResample <- vfold_cv(MSOAMean, v = 5, repeats = 20, strata = "HighVal")
set.seed(1235)
MSOAHighLUPResample <- vfold_cv(MSOAMean, v = 5, repeats = 20, strata = "HighLUP")

MSOAModels <-bind_rows(LinearClassifierCombiner(MSOAMean,  "HighVal", Formulas, MSOAHighValResample),
                       LinearClassifierCombiner(MSOAMean,  "HighLUP", Formulas, MSOAHighLUPResample))  %>%
  MarkModels()

LADModelsLin <- bind_rows(LinearClassifierCombiner(MSOAMean,  "LowUsePerc", Formulas, MSOAHighValResample, Classifier = FALSE),
                       LinearClassifierCombiner(MSOAMean,  "MedianDiff", Formulas, MSOAHighValResample, Classifier = FALSE)) %>%
  MarkModels


MSOAModTab <- MSOAModels %>% SummariseModels(Dependent, IncAfford2,  IncTourism) %>%
  mutate(Dependent = ifelse(Dependent=="HighLUP", "High LUP %", "High LUP value")) %>%
  mutate(ModelID = rep(1:4,2))

MSOAModTab %>% select(ID = ModelID, Dependent, `Beats NULL`, `Mean Accuracy`, PosPred, NegPred, `Null Difference`) %>%
  slice(c(4)) %>%
  xtable(., caption="Summary of viable MSOA authority models",
       label = "tab:MSOAmodels") %>% 
  print(., type="latex", file=file.path(TexTables,"MSOAmodels.tex"))


#Formula two provides the most accurate model without over fitting. Being a good model for identifying which LADs will have hhigh value LUPs
MSOAMODOut2 <- ExtractClassCoeffs(MSOAMean, "HighVal", MSOAHighValResample, Formulas[[2]])  %>% filter(term !="(Intercept)")
MSOAMODOut2 %>% ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
MSOAMODOut2 %>% ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

#Including both tourism and the quadratic affordability term produces the best model. Which makes sense given the plot earlier
MSOAMODOut <- ExtractClassCoeffs(MSOAMean, "HighLUP", MSOAHighLUPResample, Formulas[[4]]) %>% filter(term !="(Intercept)")
MSOAMODOut %>% ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
MSOAMODOut %>% ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

MSOAMODOut %>%
  select(-statistic, -std.error) %>%
  group_by(term) %>%
  summarise_all(funs(mean, median)) %>%
  mutate(estimate = exp(estimate_mean))

#Rubbish
MSOAModelsLin %>%
  select(-sample) %>%
  group_by(Dependent, IncAfford2, IncTourism) %>%
  summarise_if(is.numeric, mean, na.rm = T)


bind_rows(LADMODOut %>% mutate(type = "Financialised LAD"),
          LADMODOut3 %>% mutate(type = "Highly Financialised LAD"),
          LADMODOut2 %>% mutate(type = "High LUP % LAD"),
          MSOAMODOut %>% mutate(type = "High LUP % MSOA")
) %>%
  ggplot( aes(x = term, y = estimate, fill = term)) + geom_boxplot()+
  facet_wrap(~type)+
    theme(legend.position = "none", axis.text.x = element_text(angle = 15, hjust = 1))
SaveFig("ModelCoeffs.pdf")

```


Take away message is that the LAD model cannot predict LUP% whilst the MSOA model cannot predict High LUP Value

#Model Bootstrap results

using the most sucessful model formula I then use the Bootstrap data to create 501 models then majority vote the response variable.

The Bootstrap model doesn't provide any more information. despite the pertubations it tends to always predict certain LADs incorrectly and others correctly. This means that the bootstrap predictions are not sufficiently independent to make the proocess worthwhile.

```{r}
#Creates a boots model which can be analysed

set.seed(1235)
LADHighValResample200 <- vfold_cv(LADMean, v = 5, repeats = 200, strata = "HighVal")

setwd(basewd)
if(file.exists("LADBootVoteModelBig.rds")){
  
    LADBootVoteModelBig <- readRDS("LADBootVoteModelBig.rds")
  
}else{

        ModKlar <- LADModelData %>% 
          mutate(Reference = as.factor(HomesMedian < LowUseMedian)) 

 LADBootVoteModel <- (1:nrow(LADHighValResample200)) %>% map_df(~{ 
       print(.x)
        trainrows <- rsample2caret(LADHighValResample200)$index[[.x]]
        testrows <-  rsample2caret(LADHighValResample200)$indexOut[[.x]]
        
        test <- ModKlar %>% 
          BootstrapResampledModelPerf(., trainrows, 
                                      testrows, 
                                      as.formula(Reference ~ TourismDensity + AffordRatio)) # creates a dataframe out predictions
    

          Refs <- LADMean %>% 
          mutate(Reference = as.factor(HomesMedian < LowUseMedian)) %>%
              slice(testrows) %>%
              select(Reference, LAD11CD)
        
    tibble(Preds = rowSums(test>0.5), PredProbs = rowMeans(test), ID = .x) %>%
      bind_cols(Refs)
          
 })


 LADBootVoteModelBig<- LADBootVoteModel %>%
   mutate(Predres = (PredProbs>0.5)==Reference
          ) %>%
   left_join(LADRefAverages %>% select(-Afford, -Affordsd, -HighLUP)) %>%
   group_by(LAD11CD) 
 
 saveRDS(LADBootVoteModelBig, "LADBootVoteModelBig.rds")
 
 rm(LADBootVoteModel);rm(ModKlar);rm(test)
}
 
 LADBootVoteModelBig %>% ggplot(aes(x = PredProbs, y = HighVal, colour= Predres)) + geom_point()
 
 
BootRes <- (1:nrow(LADHighValResample200)) %>%
  map_df(~{
    
    test <- LADBootVoteModelBig %>%
      filter(ID==.x)
  
      ConfOut <-  confusionMatrix( data =test$PredProbs>0.5, reference = test$Reference, positive = "TRUE")
     #metrics(data = ., truth = Reference, estimate = factor(PredProbs>0.5, levels = "TRUTH", "FALSE")) 
      
       ConfOut$overall %>% t %>% data.frame() %>% as.tibble %>% 
      bind_cols(ConfOut$byClass %>% t %>% data.frame() %>% as.tibble) %>%
      mutate(sample = .x)
    
  })


BootRes %>% SummariseModels()
 
 test <- LADBootVoteModelBig %>%
   group_by(LAD11CD, Predres) %>%
   mutate(counts = n()) %>%
   summarise_all(mean) %>%
   select(-ID, -LAD11NM)
 
 test2 <- test %>%
   select(LAD11CD, Predres, counts) %>%
   spread(key = Predres, value = counts, fill = 0) %>%
   mutate(FractCorrect = `TRUE`/200) %>%
   left_join(LADMean %>% select(LAD11CD, HighValCount, LAD11NM, AffordRank, 
                                LowUseRank, LowUseAfford, TourismDensity,HighVal)) %>%
   left_join(select(LADMean,LAD11CD, HomesMedian, MedianDiff, Ego1TourismDens )) %>%
   mutate(PercDiff = (MedianDiff-HomesMedian)/HomesMedian) %>%
   left_join(LostLowUse)
 
   test2 %>%
  ggplot(aes(x= AffordRank, y = LowUseRank, colour = FractCorrect>.1)) + geom_point() +
    geom_vline(xintercept  = 0.5)+
    geom_hline(yintercept = 0.5) +
    labs(title = "Seperating Low-Use type  by affordability", x = "Affordability percentage rank", y = "Low-Use percentile rank")
  
  test2 %>%
  ggplot(aes(x= AffordRank, y = log(TourismDensity), colour = FractCorrect>.1)) + geom_point() 
  
    test2 %>%
  ggplot(aes(x= AffordRank, y = log(TourismDensity), colour = FractCorrect>.1)) + geom_point() 
   
```


##Create predictive dataset

Create the predictive data set for the local authorities in England and wales for which we do not hold data

#Create adjustment probability

```{r}
#calculate the minimum number of net swaps to move from majority to minority
CalcZ <- function(Tot,M){
  
  (2*M-Tot+1)/2
  
}

#Majority class probability function
PiGreaterXZ <- function(x,z,M, p){
  
  sum(dbinom((x+z):M, M, p))
  
}

Tot <- 501
H <-282
error <- 0.26

#Go through all vote values and calculate the probability
AdjustmentProb <- 0:501 %>% map_dbl(~{

M <-abs(Tot/2 -.x)+Tot/2
z<- CalcZ(Tot, M)


AllProb <- 0:(Tot-M)  %>% map_dbl(~{
  PiGreaterXZ(.x, z, Tot, error)*dbinom(.x, Tot-M, error)
  })

sum(AllProb)
}) %>% tibble(ConfProb = .) %>%
  mutate(Votes = 0:501,
         ConfProb = 1-ConfProb) #We are very certain these will stay as they are
```

#Predict LADs

```{r}

PredictData <-CreateGeogModelData("LAD", file.path(basewd, "BootstrapAllLADs") ) %>%  
  MeanModelData(., "LAD")


FormulaBootLAD<-as.formula(Reference ~ TourismDensity + AffordRatio + AffordRatioScale2)


ModPreds <- c("HighVal", "HighLUP", "HighFinance") %>%
  map(~{

EWMod <- LADMean %>% 
  rename_(Reference = .x )  %>% 
  mutate(Reference = as.factor(Reference)) %>%
      glm(formula = FormulaBootLAD, data = ., family=binomial(link='logit'))

    
    PredictData  %>%
      predict(EWMod, newdata = ., type = "response" ) %>%
      as.tibble(.) %>% setNames(.x)

}) %>% bind_cols()
    
EWMod2<-ModPreds %>% mutate_all(funs((.>0.5)*1)) %>%
  mutate_all(funs(ifelse(.==0, 0.2,0.8))) %>%
               mutate(LAD11CD = PredictData$LAD11CD)




WholeCountry <- LADMean %>% select(LAD11CD, HighVal = HighValCount, HighLUP ) %>%
  mutate(HighVal = HighVal/max(HighVal), #a bit dodgey to do that but I know there are 100% outcomes
         HighLUP = ifelse(HighLUP =="TRUE", 1, 0),
                  HighFinance = HighVal*HighLUP) %>% #The addujsted probability of the collected dataset is simply the probability that the medianHOMes<MedianLUPs so no change us needed
  bind_rows(EWMod2) %>%
  left_join(EW2 %>% group_by(LAD11CD, LAD11NM) %>% summarise(Pop = sum(Pop))) 

NumberOfSamples <- 10000

PopAffected <- matrix(runif(NumberOfSamples*nrow(WholeCountry)), ncol = NumberOfSamples) 

PopAffected <- tibble(HighValPop =  ((PopAffected <WholeCountry$HighVal)*WholeCountry$Pop) %>%  t %>% rowSums(.),
         HighFinancePop =  ((PopAffected <WholeCountry$HighFinance)*WholeCountry$Pop) %>%  t %>% rowSums(.)) %>%
  mutate(PopPerc = HighValPop/ sum(WholeCountry$Pop),
         FinancePopPerc = HighFinancePop/ sum(WholeCountry$Pop))

PopAffected %>% 
  select(PopPerc, FinancePopPerc) %>%
  gather(key, value) %>%
  mutate(key = ifelse(key == "PopPerc", "Financialised", "Highly Financialised")) %>%
  ggplot(aes(x = value)) + geom_density() +
  facet_wrap(~key, scales = "free_x") +
  labs(title ="Estimate percentage of the Population living in areas with high LUP values",
       x = "Population percentage")
SaveFig("PopulationHighLUP.pdf")

quantile(PopAffected$PopPerc,  probs = c(2.5 ,97.5)/100)
quantile(PopAffected$FinancePopPerc,  probs = c(2.5 ,97.5)/100)

```


#Plot maps


##Model Outputs
```{r}
LADshapfile <-file.path(basewd,"ShapeFiles", "Local_Authority_Districts_December_2014_Ultra_Generalised_Clipped_Boundaries_in_Great_Britain",
                        "Local_Authority_Districts_December_2014_Ultra_Generalised_Clipped_Boundaries_in_Great_Britain.shp")

LADShape <- st_read(LADshapfile)

LADShape2 <- WholeCountry %>%
  mutate(HighVal = HighVal>0.5,
         HighLUP = HighLUP>0.5,
         HighFinance = HighFinance>0.5) %>% 
  select(LAD11CD:HighFinance) %>%
  gather(key = Type, value = Status, -LAD11CD) %>%
  mutate(Type = case_when(
    Type =="HighFinance" ~ "Highly Financialized",
    Type == "HighLUP" ~"Large LUP percentage",
    Type == "HighVal" ~"Some Financialization"
  ) %>% as.factor(.) %>% fct_relevel("Highly Financialized", after = 2),
  Status = as.factor(Status) %>% fct_relevel(TRUE, after = 0)) %>%
  left_join(LADShape,., by = c("lad14cd" = "LAD11CD")
            ) %>% # Status = as.factor(Status) %>% fct_relevel(TRUE, after = 0)
  filter(!is.na(Status))

#plot and save
ggplot(LADShape2) +
  geom_sf(aes(fill = Status)) + 
  facet_wrap(~Type)+
  labs(title = "LUP and financialization status in England and Wales") +
   theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())
SaveFig("EWHighValMap.pdf")

```


#Plot LUP perc

```{r}

LSOAshape <- st_read(LSOAshapedata) %>%
  left_join(DATAdf, by = c("lsoa11cd"="LSOA11CD")) %>% filter(grepl("Kensington and Chelsea",lsoa11nm)|  grepl("Bradford",lsoa11nm),
                             LAD11CD=="E08000032"|LAD11CD=="E09000020")

colourTypes <- c( "0-10" = "#F8766D", 
                    "11-20" = "#A3A500", 
                   "21-30" ="#00BF7D", 
                   "30+" = "#00B0F6")

LowUseColours <- gg_color_hue(4)
names(LowUseColours) <- c( "0-10", "11-20","21-30","30+")

gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}


LSOAshape %>%
  filter(LAD11CD=="E09000020")%>%
ggplot(.)+  geom_sf(aes(fill = LowuseClass))  +
   theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())+
  ggtitle("Low Use percentage by LSOA in Kensington and Chelsea")
SaveFig("ChelseaMap.pdf")                       


LSOAshape %>%
  filter(LAD11CD=="E08000032")%>%
ggplot(.)+  geom_sf(aes(fill = LowuseClass))  +
      scale_fill_manual(values = LowUseColours) +
   theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank())+
  ggtitle("Low Use percentage by LSOA in Bradford")
SaveFig("BradfordMap.pdf")                       


```


#MSOA model

Not Neccesary

```{r}
# 
# MSOAshapefile <-file.path(basewd,
#                          "/ShapeFiles/Middle_Layer_Super_Output_Areas_December_2011_Super_Generalised_Clipped_Boundaries_in_England_and_Wales",
#                          "Middle_Layer_Super_Output_Areas_December_2011_Super_Generalised_Clipped_Boundaries_in_England_and_Wales.shp")
# 
# MSOAShape <- st_read(MSOAshapefile) %>% left_join(MSOAWholeCountry, by = c("msoa11cd" = "MSOA11CD")) %>%
#   filter(!is.na(HighLUP))
# 
# 
# ggplot(MSOAShape2) +
#   geom_sf(aes(fill = HighLUP, colour = HighLUP)) + 
#   labs(title = "Local Authorities classified as having above the median percentage of LUPs")
# SaveFig("EWHighLUPMap.pdf")

```

