---
title: "Untitled"
author: "Jonathan Bourne"
date: "9 August 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---




```{r}
SubCode <- "~/Dropbox/SSE/Empty Homes/EmptyHomesCode/SubCode"
setwd(SubCode)
source("Setup.R")

setwd(Functions)
list.files() %>% map(source)

Figures <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Figures" #file.path(basewd, "Figures")
TexTables <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Tables"
source(file.path(CommonCode, "AuxdataLoad.R"))
```


#Process all areas

This chunk processes all the areas and combines into a single dataframe
It also cleans all postcodes that are in other LADs out... should it just combine them into the other LADs LSOA?
```{r}
#creates a data frame called DATAdf
source(file.path(SubCode, "LOADandProcessLADData.R"))

```

#MSOA to LAD
```{r}
MSOAtoLAD <- EW2 %>%
  group_by(MSOA11CD, LAD11CD, LAD11NM) %>%
  summarise(TotMSOA = n(),
         Homes = sum(Homes),
         Region = first(Region)) %>%
  arrange(-TotMSOA) %>%
  group_by(MSOA11CD) %>%
  mutate(counts = n()) %>%
  summarise(LAD11CD = first(LAD11CD),
            TotMSOA = first(TotMSOA),
            counts = first(counts),
            LAD11NM = first(LAD11NM),
            Homes = first(Homes),
            Region = first(Region))
```


#Get Population Statistics

This chunk calculates how much data cover I have using several metrics

```{r}
DataCover <-DATAdf  %>%
                #filter(!is.na(LAD11NM)) %>%
  summarise(
           'LADs' = length(unique(DATAdf$LAD11CD)), 
           '% of all LADs' = length(unique(DATAdf$LAD11CD))/length(unique(EW2$LAD11CD))*100,
           LUPs = sum(LowUse, na.rm = T),
           Homes = sum(Homes, na.rm = T),
           '% of all Homes' = Homes/sum(EW2$Homes)*100,
           Population = sum(Pop, na.rm= T),
           '% of total population' = Population/sum(EW2$Pop)*100) %>%
  mutate_if(.<1, funs(signif(.,3))) %>%
  mutate_if(.>1, as.integer) %>%
  mutate_all(as.character) %>% 
  gather 

DataCover 

xtable(DataCover, caption="Summary of collected data coverage",
       label = "tab:DataCover") %>% 
  print(., type="latex", file=file.path(TexTables,"DataCover.tex"))

```

#BootStrap

This section bootstraps the price classes for each property then aggregates it by quartiles per LAD
From this we can find out if the majority of a LADs Low Use properties are investment or not by comparing the median values.

The same can be done at MSOA level but the areas are so small that you don't get enough variation in the house price levels to get a meaningful result. A better way to do this would be to use the ego network of MSOA around a target to find the "Neighbourhood LAD" and see if that is investment. However as I don't have the the low use property data for all adjoining MSOA this is not possible.


##Quartiles Bootstrap

Needed for creating the skew plots
```{r}

setwd("/home/jonno/Dropbox/SSE/Empty Homes/BootstrapQuartiles")
BootstrapAllData(DATAdf, LimitValue = 1.3e5, Reps = 501)
BootStrapRes <- LoadBootstrapData()

```

##MSOA Bootstrap
Used for most of the analysis
```{r}

setwd("/home/jonno/Dropbox/SSE/Empty Homes/BootstrapMSOA2")
BootstrapAllData(DATAdf, LimitValue = 1.3e5, Reps = 501, GroupingVars = "MSOA11CD")

#Temporay untill the proper one is complete
setwd("/home/jonno/Dropbox/SSE/Empty Homes/BootstrapMSOA2")
BootStrapRes <- LoadBootstrapData()

```

#Low Use quartile plot

This chunk plots which of the LADs affordability quartiles the LUPs fall into.

##This will need to be re-worked when the quartiles are put in and the data is re bootstrapped

```{r}
CleanForPlotClassDiff <- function(df){ #not a seperate function for ease of editing
  df %>% 
  group_by(ID, class) %>%
  summarise(Homes = sum(Homes), LowUse = sum(LowUse)) %>%
  group_by(ID)%>%
  mutate(RatioExvsAct = LowUse/sum(LowUse)) %>%
  mutate(RatioExvsAct = (RatioExvsAct/0.25)-1)
}

#I think I should have 1 plot with high value skew, one plot with low value skew and 1 plot for all data
#These three plots should be shown as a panel of data

CleanForPlotClassDiff(BootStrapRES$ChelseaLON)  %>%
PlotClassDiff(., "Kensington and Chelsea")
SaveFig("ClassdiffChelsea.pdf")

CleanForPlotClassDiff(BootStrapRES$BarrowCUMBRIA) %>% #maybe choose a better place than barrow?
PlotClassDiff(., "Barrow")
SaveFig("ClassdiffBarrow.pdf")


#Plot for all data
All_data_class <- BootStrapRES %>% 
  bind_rows %>%
  select(CountryClass, ID, LowUse, Homes) %>%
  rename(class = CountryClass)%>%
  CleanForPlotClassDiff

All_data_class %>% group_by(class) %>%
  summarise_all(mean)


PlotClassDiff(All_data_class)
SaveFig("ClassdiffAllData.pdf")

```

#Tourism data
This section Loads and processes tourism Data


```{r}

CorePstCd <- PstCdLSOA.raw %>%
  left_join(EW2, by = c("lsoa11cd" = "ECODE")) %>%
  select(Postcode,LSOA11CD= lsoa11cd, LAD11CD, LAD11NM, MSOA11CD, Pop)

setwd(file.path(basewd, "VOA data"))

#Creates Tourism dataframe at LSOA level
Tourism <- read_delim("uk-englandwales-ndr-2017-listentries-compiled-epoch-0002-baseline-csv.csv", 
                          delim = "*", col_names = F) %>%
  mutate(X15 = gsub(" ", "", X15)) %>%
  left_join(., CorePstCd, by = c("X15" = "Postcode")) %>%
  filter(!is.na(LAD11CD)) %>%
  filter(grepl("(SELF CATER)|(HOLIDAY)|(HOTEL)|(HOSTEL)|(GUEST)",X6))  %>%
  mutate(Guest = grepl("(SELF CATER)|(HOLIDAY)|(GUEST)",X6),
         Hotel = grepl("(HOTEL)|(HOSTEL)",X6)) %>%
   group_by(LSOA11CD, X6) %>%
  summarise(Tourism = n(),
            LAD11CD = first(LAD11CD),
            LAD11NM = first(LAD11NM),
            MSOA11CD = first(MSOA11CD),
            Pop = first(Pop),
            Guest = sum(Guest),
            Hotel = sum(Hotel)) 

```


#Create LAD and MSOA DATA frames

This chunk creates useful data frames for the rest of the analysis

##LAD Data

HArtlePool E06000001
COl E02000001
```{r}
LADModelData <-CreateGeogModelData("LAD", "/home/jonno/Dropbox/SSE/Empty Homes/BootstrapMSOA")
LADMean<- MeanModelData(LADModelData, "LAD") %>%
  mutate(AffordRank2 = scale(AffordRank)^2,
         TourismDensityRank = rank(TourismDensity, ties.method = "max"))

 
  LADMean %>%
  ggplot(aes(x= AffordRank, y = TourismDensityRank, colour = HighVal)) + geom_point()
 
  LADMean %>%
  ggplot(aes(x= MedianDiffRank, y = LowUseRank, colour = HighVal)) + geom_point()
 
 
```



##MSOA Data

```{r}

MSOAModelData <-CreateGeogModelData("MSOA", "/home/jonno/Dropbox/SSE/Empty Homes/BootstrapMSOA") 
MSOAMean <- MeanModelData(MSOAModelData, "MSOA") %>%
  left_join(select(MSOAtoLAD, MSOA11CD, LAD11CD)) %>%
  mutate(AffordRank2 = scale(AffordRank)^2,
         TourismDensityRank = rank(TourismDensity, ties.method = "max")) 
 
 MSOAMean %>%
  ggplot(aes(x= AffordRank, y = TourismDensity, colour = HighVal)) + geom_point()
 
  MSOAMean %>%
  ggplot(aes(x= MedianDiffRank, y = LowUseRank, colour = HighVal)) + geom_point()
 
  #Great results for the MSOA model predicting High LUP numbers
  ModelFormula <- as.formula(Reference ~ TourismRank+ AffordRank+ AffordRank2)
  
  
```

#Price affordability correlation

Why is there such a large difference between price and affordability at lad level and MSOA level  is this a mistake?
```{r}

cor(LADMean$AffordRatio, LADMean$HomesMedian)
cor(MSOAMean$AffordRatio, MSOAMean$HomesMedian)
```


#Affordability

This chunk includes affordability and plots the theoretical and observed curves.
Uses the Bootstrapped values to calculate the affordability

```{r}
MSOAAffordRatio <- MSOAMean %>%
  left_join(select(MSOAtoLAD, MSOA11CD, Region), by ="MSOA11CD")

  
MSOAAffordRatio  <- bind_rows(MSOAAffordRatio %>% filter(Region != "London") %>% mutate(type = "Excluding London"), 
                    MSOAAffordRatio %>% mutate( type = "Including London"),
                    MSOAAffordRatio %>% filter(Region == "London") %>% mutate( type = "Only London")) %>%
    group_by(type) %>%
    mutate(
      decile = ntile(AffordRatio, 100)) %>%
      group_by(decile, type) %>%
    summarise(mean = mean(LowUsePerc),
              median = median(LowUsePerc),
              counts = n(),
              AffordRatio= mean(AffordRatio)
              ) 


MSOAAffordRatio %>% 
  #filter(type == "Excluding London") %>% 
  ggplot(aes(x= decile, y = median)) + 
  facet_grid(.~type) + 
  #geom_point(data =unaffordExLon , aes(x= decile, y = LowUsePerc), alpha = 0.2 )+
  geom_point(aes(colour = type))  +
  labs(x = "2 percentile group", y = "median Percent Low Use") + 
  theme(legend.position= "none") + 
  geom_smooth(method="loess",se=FALSE, colour = "black") +
 #coord_cartesian(ylim = c(0.01, 0.05)) +
   scale_y_continuous(labels = scales::percent)
SaveFig("LowUsePercVSratio.pdf")


#The plot of the theoretical curves
data_frame(Index = 1:100) %>%
  mutate(Primary = exp(-0.05*(Index))+0.05, 
         Auxilary = (exp(Index/100)-1),
    Total = Primary + Auxilary) %>%
  gather(key = "Type", value = "value", -Index) %>%
  ggplot(aes(x= Index, y = value, colour = Type )) +
  geom_line() +
  labs(title = "LUP percentage as a function of primary and Auxilary demand",
       x = "Demand",
       y = "LUP percentage") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank()
  )
SaveFig("TheoreticalDemand.pdf")

```



#Models

The models that will be made are all binary logistic regression

Are the majority of the of the LUPS above the LAD price? 
  this uses the variables Affordability Rank (normalised), the square of the affordability rank and tourism
Are there more than the average LUPS in the LAD

Are there more than average LUPS in the LSOA/MSOA

Both models can be bootstrapped as the investment number relies on the inferred value of Homes and LUPs and the affordability relies on the bootstrapped Homes price... only question is is there enough variation in the median homes price for it to be worth it.



#LAD Model

works well at predicting high numbers of LUPs
 ModelFormula <- as.formula(Reference ~ TourismDensityRank + AffordRank)

```{r}

set.seed(1983)
TestResample <- 1:100 %>% map(~  resample_partition(LADMean , c(test = 0.2, train = 0.8)))

#Make a lot of models 

Combos<-expand.grid(1:2,1:2)

LADModels <- map2_df(.x = c("HighVal", "HighLUP")[Combos[,1]], .y = c(as.formula(Reference ~ HomesMedianRank+ AffordRank), 
                                                         as.formula(Reference ~ TourismDensityRank+ AffordRank))[Combos[,2]],
        ~{
          print(paste(.x,.y))
          LADMean %>% #LADMean %>%
    rename_(Reference = .x ) %>%
  ResampledModelPerf(., TestResample,  .y) %>%
            mutate(Dependent = .x,
                   Formula = toString(.y))
          
        } ) %>%
  mutate(Formula = ifelse(grepl("Tourism", Formula), "Tourism", "Price"))


MODOut <- 1:length(TestResample) %>% map_df(~{
  Mod2 <- LADMean  %>%
    rename(Reference = HighVal) %>%
  mutate(Reference = as.factor(Reference)) %>%
  slice(as.integer(TestResample[[.x]]$train)) %>%
  glm(formula = as.formula(Reference ~ TourismDensityRank+ AffordRank), data = ., family=binomial(link='logit'))%>%
    tidy

})

ggplot(MODOut, aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
  


LADModels %>%
  group_by(Dependent, Formula) %>%
  summarise(BeatNull = sum(Accuracy>AccuracyNull)/n(), #How many times did the models beat NULL
            meanAcc = mean(Accuracy), #What was the mean accuracy
            NULLMod = mean(AccuracyNull),
            DiffNull = mean(Accuracy-AccuracyNull), #Mean difference relative to NULL
            StatistSignif = t.test(Accuracy,AccuracyNull, alternative = "greater", paired = T)$p.value) #Was Beat NUll statis

```


#MSOA Model
```{r}

    #as.formula(Reference ~ AffordRank+ AffordRank2+ TourismDensityRank)

set.seed(1983)
TestResample <- 1:100 %>% map(~  resample_partition(MSOAMean , c(test = 0.2, train = 0.8)))

#Make a lot of models 

Combos <- expand.grid(1:2,1:3)

MSOAModels <- map2_df(.x = c("HighVal", "HighLUP")[Combos[,1]], .y = c(as.formula(Reference ~ HomesMedian+ AffordRank), 
                                                         as.formula(Reference ~ TourismDensityRank+ AffordRank),
                                                         as.formula(Reference ~ TourismDensityRank+ AffordRank+ AffordRank2))[Combos[,2]],
        ~{
          print(paste(.x,.y))
          MSOAMean %>% #LADMean %>%
    rename_(Reference = .x ) %>%
  ResampledModelPerf(., TestResample,  .y) %>%
            mutate(Dependent = .x,
                   Formula = toString(.y))
          
        } ) %>%
  mutate(type = ifelse(grepl("2", Formula), "2", "1"),
          Formula = ifelse(grepl("Tourism", Formula), "Tourism", "Price")
         )


MODOut <- 1:length(TestResample) %>% map_df(~{
  Mod2 <- LADMean  %>%
    rename(Reference = HighVal) %>%
  mutate(Reference = as.factor(Reference)) %>%
  slice(as.integer(TestResample[[.x]]$train)) %>%
  glm(formula = as.formula(Reference ~ TourismDensityRank+ AffordRank), data = ., family=binomial(link='logit')) %>%
    tidy

})

ggplot(MODOut, aes(x = term, y = estimate)) + geom_boxplot()


MSOAModelSumm <- MSOAModels %>%
  group_by(Dependent, Formula, type) %>%
  summarise(BeatNull = sum(Accuracy>AccuracyNull)/n(), #How many times did the models beat NULL
            meanAcc = mean(Accuracy), #What was the mean accuracy
            NULLmod = mean(AccuracyNull),
            DiffNull = mean(Accuracy-AccuracyNull), #Mean difference relative to NULL
            StatistSignif = t.test(Accuracy,AccuracyNull, alternative = "greater", paired = T)$p.value) #Was Beat NUll statis

```

