---
title: "Untitled"
author: "Jonathan Bourne"
date: "9 August 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---
Mang en natt har jeg sittet opp og skrevet dype romaner paa dette skrivebordet. Det er pent brukt, men har gitt meg graa haar. Det er som min Moby Dick, jeg har aldri helt klart aa temme det, boken er halvskrevet, hodet fult, hjertet tomt. Derfor maa det bort, kona orker ikke at jeg sitter oppe og stirrer paa uskrevne linjer i min moleskinen mer.



```{r}
SubCode <- "~/Dropbox/SSE/Empty Homes/EmptyHomesCode/SubCode"
setwd(SubCode)
source("Setup.R")

setwd(Functions)
list.files() %>% map(source)

Figures <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Figures" #file.path(basewd, "Figures")
TexTables <- "/home/jonno/Dropbox/Apps/ShareLaTeX/Empty Homes Write up 2/Tables"
suppressMessages(source(file.path(CommonCode, "AuxdataLoad.R")))
```


#Process all areas

This chunk processes all the areas and combines into a single dataframe
It also cleans all postcodes that are in other LADs out... should it just combine them into the other LADs LSOA?
```{r}
#creates a data frame called DATAdf
suppressMessages(source(file.path(SubCode, "LOADandProcessLADData.R")))
rm(OAandPstCd)
```

#MSOA to LAD
```{r}
MSOAtoLAD <- EW2 %>%
  group_by(MSOA11CD, LAD11CD, LAD11NM) %>%
  summarise(TotMSOA = n(),
         Homes = sum(Homes),
         Pop = sum(Pop),
         Region = first(Region)) %>%
  arrange(-TotMSOA) %>%
  group_by(MSOA11CD) %>%
  mutate(counts = n()) %>%
  summarise(LAD11CD = first(LAD11CD),
            TotMSOA = first(TotMSOA),
            counts = first(counts),
            LAD11NM = first(LAD11NM),
            Homes = first(Homes),
            Pop = first(Pop),
            Region = first(Region)) %>%
  mutate(Region = ifelse(is.na(Region), "Wales", Region))
```


#Get Population Statistics

This chunk calculates how much data cover I have using several metrics

```{r}
DataCover <-DATAdf  %>%
                #filter(!is.na(LAD11NM)) %>%
  summarise(
           'LADs' = length(unique(DATAdf$LAD11CD)), 
           '% of all LADs' = length(unique(DATAdf$LAD11CD))/length(unique(EW2$LAD11CD))*100,
           LUPs = sum(LowUse, na.rm = T),
           Homes = sum(Homes, na.rm = T),
           '% of all Homes' = Homes/sum(EW2$Homes)*100,
           Population = sum(Pop, na.rm= T),
           '% of total population' = Population/sum(EW2$Pop)*100) %>%
  mutate_if(.<1, funs(signif(.,3))) %>%
  mutate_if(.>1, as.integer) %>%
  mutate_all(as.character) %>% 
  gather 

DataCover 

xtable(DataCover, caption="Summary of collected data coverage",
       label = "tab:DataCover") %>% 
  print(., type="latex", file=file.path(TexTables,"DataCover.tex"))

rm(DataCover)

```


#Calculate Quantiles

This rather long winded way of getting the quartiles is important as it weights the sampling by Homes in an area. Relative proportion of sales in an area is not the same as homes. Some areas have higher turnover other lower. This means the quartiles need to be repeatedly sampled to get the right quartiles for the whole dataset.

In the below chunk I have sampled the data 501 times, however it is only necessary to sample one the quartiles don't change.

```{r}

#Create a data frame of LSO
PriceCounts <-prices %>%
  filter(X5 %in% c("D", "S", "T", "F")) %>%
  group_by(LSOA11CD) %>%
  summarise(counts = n()) %>%
  arrange(counts)
##The subsampling is 1/10 of the number of homes in each LSOA, this stops crashes
PriceCounts<- EW2 %>%
  select(LSOA11CD = ECODE, Homes, Pop, LAD11CD, MSOA11CD) %>%
  left_join(.,PriceCounts, by = "LSOA11CD") %>%
  mutate(counts = ifelse(is.na(counts), 0, counts),
         Homes2 = Homes/10) %>%
  filter(counts!=0)

#Create a list of price vectors by LSOA
#This process can take a while
setwd(basewd)
if(file.exists("LSOAPriceList2.rds")){
  
  LSOAPriceList <- readRDS("LSOAPriceList2.rds")
  
}else{
   
  LSOAPriceList <- 1:nrow(PriceCounts) %>% map(~{
   print(.x)
    prices %>%
      filter(LSOA11CD==PriceCounts$LSOA11CD[.x], 
             X5 %in% c("D", "S", "T", "F")) %>%
      pull(X2)
    })
  names(LSOAPriceList) <- PriceCounts$LSOA11CD
  saveRDS(LSOAPriceList, "LSOAPriceList2.rds")

}

if(file.exists("AllDataQuartiles.rds")){
  
  AllDataQuartiles <- readRDS("AllDataQuartiles.rds")
  
}else{
  #Subset list to only the LADs used in analyis
  LSOAPriceList2 <- LSOAPriceList[names(LSOAPriceList) %in% unique(DATAdf$LSOA11CD)]
  PriceCounts2 <- PriceCounts %>%
    filter(LSOA11CD %in% unique(DATAdf$LSOA11CD))
  
  AllDataQuartiles <- StratifiedBoot(LSOAPriceList2, 
                                     PriceCounts2$LSOA11CD, 
                                     PriceCounts2$Homes2, 
                                     samples = 501) #change to 1 for a quicker solution has no effect on outcome
  
  saveRDS(AllDataQuartiles, "AllDataQuartiles.rds")
  
  rm(list = c("LSOAPriceList2", "PriceCounts2"))

}

AllDataQuartilesSummary <- AllDataQuartiles %>%
  summarise_all(funs(mean, sd))

BasicQuartiles <- prices$X2 %>% .[prices$LAD11CD %in% unique(DATAdf$LAD11CD)] %>% quantile(.)

#difference between the two up to 8% this has knock on effects later in the analysis if the quartiles are not calculated correctly
AllDataQuartilesSummary[2:4]/BasicQuartiles[2:4]

#Add the balanced quartiles on to the prices.
prices <- prices %>%
  mutate(CountryClass = cut(X2, AllDataQuartilesSummary[2:4] %>% c(0,., Inf), 
                              labels =     c("Lower", "Lower-Mid", "Upper-Mid", "Upper"), 
                              right = F) %>% fct_relevel(., "Upper", after = 3)) 

rm(BasicQuartiles);rm(AllDataQuartiles);rm(AllDataQuartilesSummary);rm(LSOAPriceList)
   
```


#BootStrap

This section bootstraps the price classes for each property then aggregates it by quartiles per LAD
From this we can find out if the majority of a LADs Low Use properties are investment or not by comparing the median values.

The same can be done at MSOA level but the areas are so small that you don't get enough variation in the house price levels to get a meaningful result. A better way to do this would be to use the ego network of MSOA around a target to find the "Neighbourhood LAD" and see if that is investment. However as I don't have the the low use property data for all adjoining MSOA this is not possible.

##Quartiles Bootstrap

Needed for creating the skew plots
```{r}
setwd(file.path(basewd, "BootstrapQuartiles"))
BootstrapAllData(DATAdf, LimitValue = 1.4e5, Reps = 501)
BootStrapQuartiles <- LoadBootstrapData()

```

##MSOA Bootstrap
Used for most of the analysis, doesn't need to be loaded as it is loaded by other functions
```{r}
setwd(file.path(basewd, "BootstrapMSOA"))
BootstrapAllData(DATAdf, LimitValue = 1.4e5, Reps = 501, GroupingVars = "MSOA11CD")

```

##Non-Dataset LADs
This bootstrapping is done to get the data needed to predict across the whole of Wngland and Wales
```{r}
#Filter out all the LADs for the data we have, ake dataframe that can be sampled
AllOtherLADsdf <- EW2 %>% rename(LSOA11CD = ECODE) %>%
  filter(!(LAD11CD %in% DATAdf$LAD11CD)) %>%
  select(LSOA11CD:Pop, MSOA11CD, LAD11CD, LAD11NM)%>%
  mutate(LowUse = 0) 
#Bootstrap
setwd(file.path(basewd, "BootstrapAllLADs"))
BootstrapAllData(AllOtherLADsdf, LimitValue = 1.4e5, Reps = 501, GroupingVars = "MSOA11CD")

#Not needed again
rm(AllOtherLADsdf)

```



#Low Use quartile plot

This chunk plots which of the LADs affordability quartiles the LUPs fall into.

```{r}
#Plot for all data
All_data_class <- BootStrapQuartiles %>% 
  bind_rows %>%
  select(CountryClass, ID, LowUse, Homes) %>%
  rename(Class = CountryClass)

#All data summary
All_data_class %>% 
  group_by(Class,ID) %>%
  summarise_all(sum) %>%
  group_by(Class) %>%
  summarise_all(mean) %>%
  CleanForPlotClassDiff()
  
#Create multi panel plot for two areas and all data
bind_rows(
CleanForPlotClassDiff(BootStrapQuartiles$E09000020) %>% mutate(Area = "Kensington and Chelsea"),
CleanForPlotClassDiff(BootStrapQuartiles$E08000032) %>% mutate(Area = "Bradford"),
CleanForPlotClassDiff(All_data_class) %>% mutate(Area = "All Data")
) %>%
  mutate(Area = factor(Area, levels = c("Bradford", "All Data", "Kensington and Chelsea"))) %>%
  PlotClassDiff(Var = "LowUseRatio2") +
    facet_wrap(.~Area) +
  ggtitle("Difference in Low-Use Property distribution by Price Quartile")
SaveFig("ClassdiffAllData.pdf")

rm(BootStrapQuartiles);rm(All_data_class)
```

#Tourism data
This section Loads and processes tourism Data


```{r}

setwd(file.path(basewd, "VOA data"))

#Creates Tourism dataframe at LSOA level
Tourism <- read_delim("uk-englandwales-ndr-2017-listentries-compiled-epoch-0002-baseline-csv.csv", 
                          delim = "*", col_names = F) %>%
  mutate(X15 = gsub(" ", "", X15)) %>%
  left_join(., CorePstCd, by = c("X15" = "Postcode")) %>%
  #filter(!is.na(LAD11CD)) %>%
  filter(grepl("(SELF CATER)|(HOLIDAY)|(HOTEL)|(HOSTEL)|(GUEST)",X6))  %>%
  mutate(Guest = grepl("(SELF CATER)|(HOLIDAY)|(GUEST)",X6),
         Hotel = grepl("(HOTEL)|(HOSTEL)",X6)) %>%
   group_by(MSOA11CD) %>% #Have X6 as a grouping variable to 
  summarise(Tourism = n(),
            Guest = sum(Guest),
            Hotel = sum(Hotel))  %>%
  left_join(., MSOAtoLAD, by="MSOA11CD")

 MSOAAdjacency <- readShapeSpatial(file.path(basewd,"/ShapeFiles/Middle_Layer_Super_Output_Areas_December_2011_Super_Generalised_Clipped_Boundaries_in_England_and_Wales",
                                             "Middle_Layer_Super_Output_Areas_December_2011_Super_Generalised_Clipped_Boundaries_in_England_and_Wales.shp"))

 test <- poly2nb(MSOAAdjacency)
 
 MSOAAdjMat = nb2mat(test, style="B",zero.policy=T)
 
#Add MSOA codes so that they are added as the vertex names
colnames(MSOAAdjMat) <- MSOAAdjacency$msoa11cd

MSOAgraph <- graph.adjacency(MSOAAdjMat, mode = "undirected")

MSOAEgoList1 <- ego(MSOAgraph, 1) %>%map(~names(.x))
names(MSOAEgoList1) <- get.vertex.attribute(MSOAgraph, "name")

MSOAEgoList2 <- ego(MSOAgraph, 2) %>%map(~names(.x))
names(MSOAEgoList2) <- get.vertex.attribute(MSOAgraph, "name")


 TourismMSOAEgo2 <- MSOAEgoList2 %>% map_df(~{
    Tourism %>%
      filter(MSOA11CD %in% .x) %>%
      summarise(Ego2Tourism = sum(Tourism),
                Ego2Guest = sum(Guest),
                Ego2Hotel = sum(Hotel),
                Ego2Homes = sum(Homes))
 }) %>% mutate(Ego2TourismDens = Ego2Tourism/Ego2Homes,
               Ego2GuestDens = Ego2Guest/Ego2Homes,
               Ego2HotelDens = Ego2Hotel/Ego2Homes,
               MSOA11CD = names(MSOAEgoList1))
 

 
 TourismMSOAEgo1 <- MSOAEgoList1 %>% map_df(~{
    Tourism %>%
      filter(MSOA11CD %in% .x) %>%
      summarise(Ego1Tourism = sum(Tourism),
                Ego1Guest = sum(Guest),
                Ego1Hotel = sum(Hotel),
                Ego1Homes = sum(Homes))
 }) %>% mutate(Ego1TourismDens = Ego1Tourism/Ego1Homes,
               Ego1GuestDens = Ego1Guest/Ego1Homes,
               Ego1HotelDens = Ego1Hotel/Ego1Homes,
               MSOA11CD = names(MSOAEgoList1))

Tourism <- Tourism %>%
  left_join(TourismMSOAEgo1, ., by = "MSOA11CD") %>%
  left_join(., TourismMSOAEgo2, by = "MSOA11CD") %>%
  mutate(Tourism = ifelse(is.na(Tourism), 0, Tourism),
         Hotel = ifelse(is.na(Hotel), 0, Hotel),
         Guest = ifelse(is.na(Guest), 0, Guest))

rm(MSOAAdjacency);rm(test);rm(MSOAAdjMat); rm(MSOAgraph); rm(MSOAEgoList1);rm(MSOAEgoList2); rm(TourismMSOAEgo1);rm(TourismMSOAEgo2)

```


#Create LAD and MSOA DATA frames

This chunk creates useful data frames for the rest of the analysis

##LAD Data

```{r}
LADModelData <-CreateGeogModelData("LAD", file.path(basewd, "BootstrapMSOA"))
LADMean <- MeanModelData(LADModelData, "LAD") %>%
  mutate(TourismDensityRank = rank(TourismDensity, ties.method = "max"),
         LowUseType =case_when(
    "TRUE"==HighLUP & "TRUE" ==HighVal ~ 1, # All high
    "TRUE" == HighLUP & "FALSE" ==HighVal ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & "TRUE" ==HighVal ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & "FALSE"==HighVal ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
  LowUseAfford =case_when(
    "TRUE"==HighLUP & AffordRank>=0.5 ~ 1, # All high
    "TRUE" == HighLUP & AffordRank<0.5 ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & AffordRank>=0.5 ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & AffordRank<0.5 ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor)  %>%
  left_join(MSOAtoLAD %>%
              group_by(LAD11CD) %>%
              summarise(LAD11NM = first(LAD11NM)), by = "LAD11CD")

#Amount of variance in the bootstrap samples 
#HighVal varies whilst HighLUP is fixed. However, in both of them the variables change as the affordability ratio varies. Not sure if it varies enough to change much though!
LADRefAverages <- LADModelData %>%
  group_by(LAD11CD) %>%
  summarise(HighVal = mean(ifelse(HighVal==TRUE,1,0)),
            HighLUP = mean(ifelse(HighLUP==TRUE,1,0)),
            Afford = mean(AffordRatio),
            Affordsd = sd(AffordRatio)) %>%
  left_join(., MSOAtoLAD %>% select(LAD11CD, LAD11NM)%>% distinct)

#example Affordatio bootstrap data for Chelsea. Variaince is very low
LADModelData %>% filter(LAD11CD=="E09000020") %>% ggplot(aes(x = AffordRatio)) + geom_density()

  LADMean %>%
  ggplot(aes(x= AffordRank, y = LowUseRank, colour = LowUseAfford)) + geom_point() +
    geom_vline(xintercept  = 0.5)+
    geom_hline(yintercept = 0.5) +
    labs(title = "Seperating Low-Use type  by affordability", x = "Affordability percentage rank", y = "Low-Use percentile rank")
  SaveFig("LowUseAffordcolour.pdf")
 
 
```



##MSOA Data

```{r}
MSOAModelData <-CreateGeogModelData("MSOA", file.path(basewd, "BootstrapMSOA")) 
MSOAMean <- MeanModelData(MSOAModelData, "MSOA") %>%
  left_join(select(MSOAtoLAD, MSOA11CD, LAD11CD)) %>%
  mutate(AffordRank2 = scale(AffordRank)^2,
         AffordRatio2 = scale(AffordRatio)^2,
         TourismDensityRank = rank(TourismDensity, ties.method = "max")) %>%
  left_join(select(LADMean, LAD11CD, LADHighLUP = HighLUP, LADHighVal = HighVal, #Add in Saucy LAD variables
                   LADAffordRatio = AffordRatio, LADAffordRatio2 = AffordRatioScale2,
                   LADTourismDensity = TourismDensity, LADMedianDiff = MedianDiff, LADMeanMedianRatio = MeanMedianRatio,
                   LowUseType), by = "LAD11CD") %>%
  mutate(BothHighLUP =case_when(
    "TRUE"==HighLUP & "TRUE" ==LADHighLUP ~ 1, # all high
    "TRUE" == HighLUP & "FALSE" ==LADHighLUP ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & "TRUE" ==LADHighLUP ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & "FALSE"==LADHighLUP ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
  BothHighVal =case_when(
    "TRUE" ==HighVal & "TRUE" ==LADHighVal ~ 1, # all high
   "TRUE" == HighVal & "FALSE"==LADHighVal ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighVal & "TRUE" ==LADHighVal ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighVal & "FALSE"==LADHighVal ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor,
  MSOALUPLADVal =case_when(
    "TRUE" ==HighLUP & "TRUE" ==LADHighVal ~ 1, # all high
   "TRUE" == HighLUP & "FALSE"==LADHighVal ~ 2, #High LUP in MSOA but not in LAD
    "FALSE"==HighLUP & "TRUE" ==LADHighVal ~ 3, #High LUP in LAD but not MSOA
    "FALSE"==HighLUP & "FALSE"==LADHighVal ~ 4 #LOW LUP in both LAD and MSOA
  ) %>% as.factor
           )
 
```
 
 
Plots that help explain the data seperation
```{r}
bind_rows(MSOAMean %>% mutate(type = "MSOA"), LADMean %>% mutate(type = "LAD"))  %>%group_by(type)%>% mutate(Eg2 = percent_rank(Ego2TourismDens)) %>% ungroup %>%
  ggplot(aes(x= AffordRank, y = log(TourismDensity), colour = HighLUP)) + geom_point() +
  facet_wrap(~type) + 
  labs(title = "Does the geography have above the median LUPs", x = "Affordability Rank", y = "Tourism Rank") +
  guides(fill=guide_legend(title="Above Median"))
SaveFig("HighLUP.pdf")

bind_rows(MSOAMean %>% mutate(type = "MSOA"), LADMean %>% mutate(type = "LAD")) %>%
  ggplot(aes(x= AffordRank, y = TourismDensity, colour = HighVal)) + geom_point() +
  facet_wrap(~type) + 
  labs(title = "Is the median LUP price higher than the Median price of all properties", x = "Affordability Rank", y = "Tourism Density") +
  guides(fill=guide_legend(title="Above Median"))
SaveFig("HighVal.pdf")

quantile(MSOAMean$LowUsePerc)

```

#Rural Analysis

The rural areas seem to be much less likely to had a negative mean difference,  but there is not enough data for the data to be used for moddelling
```{r}
LADrural <- rural %>% group_by(LAD11CD, Urban) %>%
  summarise(count = n()) %>%
  spread(key = Urban, value = count, fill = 0) %>%
  mutate(UrbanPerc = Urban/(Urban+Rural),
         Class = ifelse(UrbanPerc>0.5, "Urban", "Rural")) %>%
  left_join(LADMean,.)


MSOArural <- rural %>% group_by(MSOA11CD, Urban) %>%
  summarise(count = n()) %>%
  spread(key = Urban, value = count, fill = 0) %>%
  mutate(UrbanPerc = Urban/(Urban+Rural),
         Class = ifelse(UrbanPerc>0.5, "Urban", "Rural")) %>%
  left_join(MSOAMean,.)

#Almost no rural areas are classified as having a negative median difference
table(LADrural$Class,LADrural$MedianDiff>0.5)
table(LADrural$Class,LADrural$LowUseType)
2/26
42/86

table(ifelse(percent_rank(LADrural$TourismDensity)>0.5, "HighTour", "LowTour"),LADrural$HighLUP,LADrural$Class)

LADrural  %>%
  ggplot(aes(x = log(LowUsePerc), y =  log(TourismDensity), colour = Class)) + geom_point()


rm(LADrural);rm(MSOArural);rm(rural)
```


#Affordability

This chunk includes affordability and plots the theoretical and observed curves.
Uses the Bootstrapped values to calculate the affordability

```{r}
MSOAAffordRatio <- MSOAMean %>%
  left_join(select(MSOAtoLAD, MSOA11CD, Region), by ="MSOA11CD")

cor(MSOAMea)

  
MSOAAffordRatio  <- bind_rows(MSOAAffordRatio %>% filter(Region != "London") %>% mutate(type = "Excluding London"), 
                    MSOAAffordRatio %>% mutate( type = "Including London"),
                    MSOAAffordRatio %>% filter(Region == "London") %>% mutate( type = "Only London")) %>%
    group_by(type) %>%
    mutate(
      decile = ntile(AffordRatio, 100)) %>%
      group_by(decile, type) %>%
    summarise(mean = mean(LowUsePerc),
              median = median(LowUsePerc),
              counts = n(),
              AffordRatio= mean(AffordRatio)
              ) 


MSOAAffordRatio %>% 
  #filter(type == "Excluding London") %>% 
  ggplot(aes(x= decile, y = median)) + 
  facet_grid(.~type) + 
  #geom_point(data =unaffordExLon , aes(x= decile, y = LowUsePerc), alpha = 0.2 )+
  geom_point(aes(colour = type))  +
  labs(x = "2 percentile group", y = "median Percent Low Use") + 
  theme(legend.position= "none") + 
  geom_smooth(method="loess",se=FALSE, colour = "black") +
 #coord_cartesian(ylim = c(0.01, 0.05)) +
   scale_y_continuous(labels = scales::percent)
SaveFig("LowUsePercVSratio.pdf")


#The plot of the theoretical curves
data_frame(Index = 1:100) %>%
  mutate(Primary = exp(-0.05*(Index))+0.05, 
         Auxilary = (exp(Index/100)-1),
    Total = Primary + Auxilary) %>%
  gather(key = "Type", value = "value", -Index) %>%
  ggplot(aes(x= Index, y = value, colour = Type )) +
  geom_line() +
  labs(title = "LUP percentage as a function of primary and Auxilary demand",
       x = "Demand",
       y = "LUP percentage") +
  theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank()
  )
SaveFig("TheoreticalDemand.pdf")

```



#Models

The models that will be made are all binary logistic regression

Are the majority of the of the LUPS above the LAD price? 
  this uses the variables Affordability Rank (normalised), the square of the affordability rank and tourism
Are there more than the average LUPS in the LAD

Are there more than average LUPS in the LSOA/MSOA

Both models can be bootstrapped as the investment number relies on the inferred value of Homes and LUPs and the affordability relies on the bootstrapped Homes price... only question is is there enough variation in the median homes price for it to be worth it.

##Model Parameters

The below are the combinations of formula and outcome variable

Including Mean Median is useful for LADs where the intra lad variance is large but not for MSOA where the whole area can have a high number of LUPs not just a part which occurs in lads. The effect on the lad model is to increase accuracy by only 1% 

```{r}
Formulas <- c(as.formula(Reference ~ AffordRatio),
              as.formula(Reference ~ TourismDensity + AffordRatio),
              as.formula(Reference ~ AffordRatio + AffordRatioScale2),
              as.formula(Reference ~ TourismDensity+ AffordRatio + AffordRatioScale2))

#the below function is used to clearly mark the model summaries with the formular type
MarkModels <- function(df){
  df %>%
  mutate(IncAfford2 = grepl("AffordRatioScale2", Formula),
        # IncAfford3 = grepl("3", Formula),
           IncTourism = grepl("Tourism", Formula)
         )
  
}

```



##LAD Model

```{r}
#Create sample
set.seed(1235)
LADHighValResample <- vfold_cv(LADMean, v = 5, repeats = 20, strata = "HighVal")
set.seed(1235)
LADHighLUPResample <- vfold_cv(LADMean, v = 5, repeats = 20, strata = "HighLUP")


#Create a load of classifiers
LADModels <- bind_rows(LinearClassifierCombiner(LADMean,  "HighVal", Formulas, LADHighValResample),
                       LinearClassifierCombiner(LADMean,  "HighLUP", Formulas, LADHighLUPResample)) %>%
MarkModels

#Create a load of linear models
LADModelsLin <- bind_rows(LinearClassifierCombiner(LADMean,  "LowUsePerc", Formulas, LADHighValResample, Classifier = FALSE),
                       LinearClassifierCombiner(LADMean,  "MedianDiff", Formulas, LADHighValResample, Classifier = FALSE)) %>%
MarkModels()

LADModels %>% SummariseModels(Dependent, IncAfford2,  IncTourism) %>%
  mutate(Dependent = ifelse(Dependent=="HighLUP", "High LUP %", "High LUP value")) %>%
  mutate(ModelID = rep(1:4,2))

#Linear models are rubbish
LADModelsLin %>%
  select(-sample) %>%
  group_by(Dependent, IncAfford2, IncTourism) %>%
  summarise_if(is.numeric, funs(mean))

#Formula two provides the most accurate model without over fitting. Being a good model for identifying which LADs will have high value LUPs
LADMODOut <- ExtractClassCoeffs(LADMean, "HighVal", LADHighValResample, Formulas[[2]]) %>% filter(term !="(Intercept)")
LADMODOut %>% ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
LADMODOut %>% ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

#Explore the best classifier
#There The quadtratic relationship is accurately covered using this formula 
LADMODOut2 <- ExtractClassCoeffs(LADMean, "HighLUP", LADHighLUPResample, Formulas[[4]]) %>% filter(term !="(Intercept)")
LADMODOut2 %>%  ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
LADMODOut2 %>%  ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

LADMODOut %>%  select(-statistic, -std.error) %>% group_by(term) %>% summarise_all(funs(mean, median))
LADMODOut2 %>%  select(-statistic, -std.error) %>% group_by(term) %>% summarise_all(funs(mean, median))

```


##MSOA Model
```{r}

set.seed(1235)
MSOAHighValResample <- vfold_cv(MSOAMean, v = 5, repeats = 20, strata = "HighVal")
set.seed(1235)
MSOAHighLUPResample <- vfold_cv(MSOAMean, v = 5, repeats = 20, strata = "HighLUP")

MSOAModels <-bind_rows(LinearClassifierCombiner(MSOAMean,  "HighVal", Formulas, MSOAHighValResample),
                       LinearClassifierCombiner(MSOAMean,  "HighLUP", Formulas, MSOAHighLUPResample))  %>%
  MarkModels()

LADModelsLin <- bind_rows(LinearClassifierCombiner(MSOAMean,  "LowUsePerc", Formulas, MSOAHighValResample, Classifier = FALSE),
                       LinearClassifierCombiner(MSOAMean,  "MedianDiff", Formulas, MSOAHighValResample, Classifier = FALSE)) %>%
  MarkModels


MSOAModels %>% SummariseModels(Dependent, IncAfford2,  IncTourism) %>%
  mutate(Dependent = ifelse(Dependent=="HighLUP", "High LUP %", "High LUP value")) %>%
  mutate(ModelID = rep(1:4,2))



#Formula two provides the most accurate model without over fitting. Being a good model for identifying which LADs will have hhigh value LUPs
MSOAMODOut2 <- ExtractClassCoeffs(MSOAMean, "HighVal", HighValResample, Formulas[[4]])  %>% filter(term !="(Intercept)")
MSOAMODOut2 %>% ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
MSOAMODOut2 %>% ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

#Including both tourism and the quadratic affordability term produces the best model. Which makes sense given the plot earlier
MSOAMODOut <- ExtractClassCoeffs(MSOAMean, "HighLUP", MSOAHighLUPResample, Formulas[[2]]) %>% filter(term !="(Intercept)")
MSOAMODOut %>% ggplot( aes(x = term, y = estimate)) + geom_boxplot() #Model Coefficients
MSOAMODOut %>% ggplot( aes(x = term, y = p.value)) + geom_boxplot() #Model p.values

MSOAMODOut %>%
  select(-statistic, -std.error) %>%
  group_by(term) %>%
  summarise_all(funs(mean, median))

#Rubbish
MSOAModelsLin %>%
  select(-sample) %>%
  group_by(Dependent, IncAfford2, IncTourism) %>%
  summarise_if(is.numeric, mean, na.rm = T)

```


Take away message is that the LAD model cannot predict LUP% whilst the MSOA model cannot predict High LUP Value

#Model Bootstrap results

using the most sucessful model formula I then use the Bootstrap data to create 501 models then majority vote the response variable.
This creates a model with an overall lower accuracy but also a smaller standard deviation

```{r}

set.seed(1235)
LADHighValResample200 <- vfold_cv(LADMean, v = 5, repeats = 200, strata = "HighVal")
test <- LADMean %>% mutate(Reference = HighVal) %>%
  ResampledModelPerf(., LADHighValResample200, Formulas[[2]])

set.seed(1235)

TestResample <- vfold_cv(LADMean, v = 5, repeats = 20, strata = "HighVal")

TestResample <- vfold_cv(LADMean, v = 5, repeats = 100, strata = "HighVal")
LADModelData2 <- LADModelData %>%
  filter(ID<102)


setwd(basewd)
if(file.exists("LADBootVoteModel.rds")){

  LADBootVoteModel <- readRDS("LADBootVoteModel.rds")
  
  }else{
    LADBootVoteModel <- 1:100 %>% map_df(~{ 
       print(.x)
        trainrows <- rsample2caret(TestResample)$index[[.x]]
        testrows <-  rsample2caret(TestResample)$indexOut[[.x]]
        
        test <- LADModelData %>% 
          mutate(Reference = as.factor(HomesMedian < LowUseMedian)) %>% 
          BootstrapResampledModelPerf(., trainrows, 
                                      testrows, 
                                      as.formula(Reference ~ TourismDensity + AffordRatio)) # creates a dataframe out predictions
    
      ConfOut <- LADMean %>% mutate(Reference = factor(HomesMedian < LowUseMedian, levels = c("FALSE", "TRUE")))  %>% 
        slice(testrows) %>% pull(Reference) %>%
        confusionMatrix( data =factor( rowMeans(test) >0.5, levels = c("FALSE", "TRUE")), reference = .)
      
    
          ConfOut$overall %>% t %>% data.frame() %>% as.tibble %>%
          mutate(sample = .x)
    
    })

}

#has a lower mean but a much small standard deviation
LADBootVoteModel %>% SummariseModels() 

bind_rows(LADModels %>% 
            filter(IncAfford2 == FALSE,  IncTourism == TRUE, Dependent=="HighVal") %>% 
            select(Accuracy) %>% mutate(type = "Basic"),
          #select(test, Accuracy) %>% mutate(type = "big repeats"),
          select(LADBootVoteModel, Accuracy) %>% mutate(type = "Boot")) %>%
  ggplot(aes(x = Accuracy, colour = type)) + geom_density() +
  ggtitle("Distribution of model results. Bootstrap has much tighter distribution")

LADModels %>% 
            filter(IncAfford2 == FALSE, IncTourism == TRUE) %>% 
            select(Accuracy) %>% mutate(type = "Basic") %>% pull(Accuracy) %>%
quantile(.,  probs = c(5, 50, 95)/100)
quantile(LADBootVoteModel$Accuracy,  probs = c(5, 50, 95)/100)


#Creates a boots model which can be analysed more

set.seed(1235)
LADHighValResample200 <- vfold_cv(LADMean, v = 5, repeats = 200, strata = "HighVal")


if(file.exists("LADBootVoteModelBig.rds")){
  
    LADBootVoteModelBig <- readRDS("LADBootVoteModelBig.rds")
  
}else{

        ModKlar <- LADModelData %>% 
          mutate(Reference = as.factor(HomesMedian < LowUseMedian)) 

 LADBootVoteModel <- (1:nrow(LADHighValResample200)) %>% map_df(~{ 
       print(.x)
        trainrows <- rsample2caret(LADHighValResample200)$index[[.x]]
        testrows <-  rsample2caret(LADHighValResample200)$indexOut[[.x]]
        
        test <- ModKlar %>% 
          BootstrapResampledModelPerf(., trainrows, 
                                      testrows, 
                                      as.formula(Reference ~ TourismDensity + AffordRatio)) # creates a dataframe out predictions
    

          Refs <- LADMean %>% 
          mutate(Reference = as.factor(HomesMedian < LowUseMedian)) %>%
              slice(testrows) %>%
              select(Reference, LAD11CD)
        
    tibble(Preds = rowSums(test>0.5), PredProbs = rowMeans(test), ID = .x) %>%
      bind_cols(Refs)
          
 })


 LADBootVoteModelBig<- LADBootVoteModel %>%
   mutate(Predres = (PredProbs>0.5)==Reference
          ) %>%
   left_join(LADRefAverages %>% select(-Afford, -Affordsd, -HighLUP)) %>%
   group_by(LAD11CD) 
 
 saveRDS(LADBootVoteModelBig, "LADBootVoteModelBig.rds")
 
 rm(LADBootVoteModel);rm(ModKlar);rm(test)
}
 
 LADBootVoteModelBig %>% ggplot(aes(x = PredProbs, y = HighVal, colour= Predres)) + geom_point()
 
 
 test <- LADBootVoteModelBig %>%
   group_by(LAD11CD, Predres) %>%
   mutate(counts = n()) %>%
   summarise_all(mean) %>%
   select(-ID, -LAD11NM)
 
 test2 <- test %>%
   select(LAD11CD, Predres, counts) %>%
   spread(key = Predres, value = counts, fill = 0)%>%
   mutate(FractCorrect = `TRUE`/200) %>%
   left_join(LADRefAverages %>% select(-Afford, -Affordsd, -HighLUP)) %>%
   left_join(select(LADMean,LAD11CD, HomesMedian, MedianDiff, Ego1TourismDens )) %>%
   mutate(PercDiff = (MedianDiff-HomesMedian)/HomesMedian)
 
 test2 %>% ggplot(aes(y = FractCorrect, x = HighVal)) + geom_jitter()

  test2 %>% ggplot(aes(y = FractCorrect, x = PercDiff, colour = is.na(Ego1TourismDens))) + geom_jitter()

 
 table(test2$FractCorrect>0.5)
 
 is.na(test2$Ego1TourismDens) %>% table
  
```

Boot MSOA

```{r}
FormulaMSOA <- as.formula(Reference ~ TourismDensity + AffordRatio)

set.seed(1983)
TestResample <- vfold_cv(MSOAMean, v = 5, repeats = 20, strata = "HighLUP")

BootVoteModel <- 1:100 %>% map(~{ 
   print(.x)
    trainrows <- rsample2caret(TestResample)$index[[.x]]
    testrows <-  rsample2caret(TestResample)$indexOut[[.x]]
    
    test <- MSOAModelData %>% 
      mutate(Reference = HighLUP) %>% 
      BootstrapResampledModelPerf(., trainrows, testrows, FormulaMSOA ) # creates a dataframe out predictions

    PredVect <- factor( rowMeans(test) >0.5, levels = c("FALSE", "TRUE"))
    
  ConfOut <- MSOAMean %>% mutate(Reference = factor(HighLUP, levels = c("FALSE", "TRUE")))  %>% 
    slice(testrows) %>% pull(Reference) %>%
    confusionMatrix( data = PredVect, reference = .)
  

      ConfOut$overall %>% t %>% data.frame() %>% as.tibble %>%
      mutate(sample = .x)

})

rm(FormulaMSOA)

test<- MSOAModelData %>% group_by(ID)%>% summarise(counts= n())

```

##Create predictive dataset

Create the predictive data set for the local authorities in England and wales for which we do not hold data

#Create adjustment probability

```{r}
#calculate the minimum number of net swaps to move from majority to minority
CalcZ <- function(Tot,M){
  
  (2*M-Tot+1)/2
  
}

#Majority class probability function
PiGreaterXZ <- function(x,z,M, p){
  
  sum(dbinom((x+z):M, M, p))
  
}

Tot <- 501
H <-282
error <- 0.26

#Go through all vote values and calculate the probability
AdjustmentProb <- 0:501 %>% map_dbl(~{

M <-abs(Tot/2 -.x)+Tot/2
z<- CalcZ(Tot, M)


AllProb <- 0:(Tot-M)  %>% map_dbl(~{
  PiGreaterXZ(.x, z, Tot, error)*dbinom(.x, Tot-M, error)
  })

sum(AllProb)
}) %>% tibble(ConfProb = .) %>%
  mutate(Votes = 0:501,
         ConfProb = 1-ConfProb) #We are very certain these will stay as they are
```

#Predict LADs

```{r}

PredictData <-CreateGeogModelData("LAD", file.path(basewd, "BootstrapAllLADs") )%>%  
  MeanModelData(., "LAD")


FormulaBootLAD<- ModelFormula <- as.formula(Reference ~ TourismDensity + AffordRatio)

trainrows <- 1:nrow(LADModelData)

BootstrapLADModels <- LADModelData %>% 
  mutate(Reference = as.factor(HighVal)) %>% 
  BootstrapResampledModelPerf(., trainrows, 0 , FormulaBootLAD, ModsOut = TRUE ) # creates a list of models



VoteResults<- BootstrapLADModels %>% map(~{
 preds2 <- tibble(x = predict(.x, newdata = PredictData, type = "response" )>0.5)
    
}) %>% bind_cols() %>% rowSums(.) %>%
  tibble(Votes = .) %>%
  mutate(Prob = Votes/length(BootstrapLADModels), 
         Class = Prob>0.5,
         LAD11CD = PredictData$LAD11CD) %>%
  left_join(., AdjustmentProb, by = "Votes") %>%
  mutate(AdjustedProb = (Prob-0.5)*ConfProb+0.5)


WholeCountry <- LADMean %>% select(LAD11CD, Votes = HighValCount) %>%
  mutate(Prob = Votes/max(Votes), #a bit dodgey to do that but I know there are 100% outcomes
         AdjustedProb = Prob,
         Class = Prob>0.5) %>% #The addujsted probability of the collected dataset is simply the probability that the medianHOMes<MedianLUPs so no change us needed
  bind_rows(VoteResults) %>%
  left_join(MSOAtoLAD %>% group_by(LAD11CD, LAD11NM) %>% summarise(Pop = sum(Pop))) %>%
  mutate(Diff = Prob-AdjustedProb)

NumberOfSamples <- 10000

PopAffected <- matrix(runif(NumberOfSamples*nrow(WholeCountry)), ncol = NumberOfSamples) 

PopAffected <- ((PopAffected >WholeCountry$AdjustedProb)*WholeCountry$Pop) %>%  t %>% rowSums(.) %>%
  tibble(InvestmentPop = .) %>%
  mutate(PopPerc = InvestmentPop/ sum(WholeCountry$Pop))

PopAffected %>% ggplot(aes(x = PopPerc)) + geom_density() +
  labs(title ="Estimate percentage of the Population living in areas with high LUP values",
       x = "Population percentage")
SaveFig("PopulationHighLUP.pdf")

```

#Predict MSOA High LUP

```{r}

PredictDataMSOA <-CreateGeogModelData("MSOA", file.path(basewd, "BootstrapAllLADs") )%>%  
  MeanModelData(., "MSOA")


FormulaMSOAPredAll <- as.formula(Reference ~ TourismDensity+ AffordRatio + AffordRatioScale2)

PredictAllMSOAMod <- MSOAMean %>% 
  mutate(Reference = as.factor(HighLUP))  %>%
      glm(formula =FormulaMSOAPredAll , data = ., family=binomial(link='logit'))

summary(PredictAllMSOA)


MSOAWholeCountry <- tibble(HighLUP = predict(PredictAllMSOAMod, newdata = PredictDataMSOA, type = "response" )>0.5,
                  MSOA11CD = PredictDataMSOA$MSOA11CD) %>%
  mutate(HighLUP = as.factor(HighLUP)) %>%
  bind_rows(select(MSOAMean, MSOA11CD, HighLUP))


```


#Plot Model Outputs

##LAD model
```{r}
LADshapfile <-file.path(basewd,"ShapeFiles", "Local_Authority_Districts_December_2014_Ultra_Generalised_Clipped_Boundaries_in_Great_Britain",
                        "Local_Authority_Districts_December_2014_Ultra_Generalised_Clipped_Boundaries_in_Great_Britain.shp")

LADShape <- st_read(LADshapfile)

LADShape2 <- LADShape %>% left_join(WholeCountry, by = c("lad14cd" = "LAD11CD")) %>%
  filter(!is.na(Class))


ggplot(LADShape2) +
  geom_sf(aes(fill = Class)) + 
  labs(title = "Local Authorities classified as having predominently high value LUPs")
SaveFig("EWHighValMap.pdf")

```


#MSOA model

```{r}

MSOAshapefile <-file.path(basewd,
                         "/ShapeFiles/Middle_Layer_Super_Output_Areas_December_2011_Super_Generalised_Clipped_Boundaries_in_England_and_Wales",
                         "Middle_Layer_Super_Output_Areas_December_2011_Super_Generalised_Clipped_Boundaries_in_England_and_Wales.shp")

MSOAShape <- st_read(MSOAshapefile) %>% left_join(MSOAWholeCountry, by = c("msoa11cd" = "MSOA11CD")) %>%
  filter(!is.na(HighLUP))


ggplot(MSOAShape2) +
  geom_sf(aes(fill = HighLUP, colour = HighLUP)) + 
  labs(title = "Local Authorities classified as having above the median percentage of LUPs")
SaveFig("EWHighLUPMap.pdf")

```

